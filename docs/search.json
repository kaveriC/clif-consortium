[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "CLIF Microbiology\n\n\n\nehr-to-clif\n\n\nmicrobiology\n\n\nERD\n\n\n\n\n\n\n\nKevin Buell, Kaveri Chhikara\n\n\nJul 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing CLIF GitHub with R\n\n\n\ngithub\n\n\ncollaboration\n\n\n\n\n\n\n\nKaveri Chhikara\n\n\nJun 6, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Common Longitudinal ICU data Format",
    "section": "",
    "text": "Federated Critical Care Research\n\n\nMulticenter critical care research often relies on sharing sensitive patient data across sites, requiring complex data use agreements (DUAs) and yielding redundant work to account for diverse data infrastructures. To overcome these barriers, we present CLIF, designed specifically for observational studies of critically ill patients across multiple centres. Through CLIF, we aim to streamline data organization into a longitudinal format and establish standard vocabularies to facilitate federated analytics and improve data readability for researchers and clinicians.\n\n\nThe CLIF consortium, comprising critical care researchers from eight US academic health systems, collaboratively developed CLIF’s schema, clinical vocabularies, and “proof of concept” datasets. CLIF’s tables emphasize care processes, clinical outcomes, and granular clinical physiology measures.\n\n\n\nParticipating sites"
  },
  {
    "objectID": "posts/sipa_blog/SIPA_blog_post.html",
    "href": "posts/sipa_blog/SIPA_blog_post.html",
    "title": "Constructing an Analytic Dataset from CLIF",
    "section": "",
    "text": "Cohort Definition: Data from UCMC COVID Datamart. Adult patients (18+) admitted January 1 2020 – March 31, 2022 and who recieved life support. Life support was defined as:\n\nReceived vasoactive medications for shock, or\nReceived invasive or non-invasive mechanical ventilation, or\nReceived oxygen therapy with PaO2/FiO2 &lt; 200 (S/F &lt; 179 if no P/F measured)\n\nPatients were excluded in they were discharged from the ED (including to hospice) and if they received &lt; 6 hours of life support.\nCLIF tables used:\n\nRCLIF_limited_identifers (admission date, discharge date)\nRCLIF_encounter_demographics_dispo (age at admission, disposition)\nRCLIF_adt (location in hospital)\nRCLIF_resp_support (respiratory support device, FiO2)\nRCLIF_labs (PaO2)\nRCLIF_vitals (SpO2)\nRCLIF_medication_adm_continuous (vasopressors)\n\n\n\n\nUse RCLIF_limited_identifers to identify admissions in the time period of interest (variable: admission_dttm)\nUse RCLIF_encounter_demographics_dispo to filter for only those 18+ at time of admission (variable: age_at_admission)\nUse the ADT table to filter out those only seen in the ER (variable: dept_name)\n“Explode” the data between admission_dttm and discharge_dttm to get hourly blocks for the duration of admission\n\n\n######## Load in limited IDs for admission dates in time period\nlimited_ids = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_limited_identifers.csv\")\nlimited_ids = limited_ids.withColumn('admission_datetime',f.to_timestamp('admission_date','yyyy-MM-dd HH:mm:ss'))\nlimited_ids = limited_ids.withColumn('discharge_datetime',f.to_timestamp('discharge_date','yyyy-MM-dd HH:mm:ss'))\nlimited_ids = limited_ids.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime', 'zip_code').distinct()\n\n# Filter to time period\nlimited_ids = limited_ids.filter(((f.col('admission_datetime')&gt;='2020-03-01') & \n                   (f.col('admission_datetime')&lt;='2022-03-31')))\nlimited_ids = limited_ids.filter((f.col('discharge_datetime')&gt;=f.col('admission_datetime')))\nlimited_ids = limited_ids.filter(f.col('discharge_datetime').isNotNull())\nlimited_ids = limited_ids.filter(f.col('admission_datetime').isNotNull())\n\n\n######## Load in encounters for age and discharge disposition\ndemo_disp = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_encounter_demographics_dispo.csv\")\ndemo_disp = demo_disp.withColumn(\"age_at_admission\",demo_disp.age_at_admission.cast('double'))\ndemo_disp = demo_disp.select('C19_PATIENT_ID', 'C19_HAR_ID', 'age_at_admission', 'disposition')\n\n# Filter to adults only\ndemo_disp = demo_disp.filter(f.col('age_at_admission')&gt;=18)\n\nadults_in_time = limited_ids.join(demo_disp, on='C19_HAR_ID', how='inner')\n\n# Exclude people only in ER\nadt = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_adt.csv\")\n\nadult_ids = adults_in_time.select('C19_HAR_ID')\nadt_adults = adult_ids.join(adt, on='C19_HAR_ID', how='inner')\nadt_adults = adt_adults.select('C19_HAR_ID', 'dept_name').distinct()\nadt_adults = adt_adults.withColumn('count', f.lit(1))\nadt_adults_wide = adt_adults.groupBy('C19_HAR_ID').pivot('dept_name').agg(f.sum('count'))\nadt_adults_wide = adt_adults_wide.filter(f.col('ICU').isNotNull() |\n                                         f.col('NA').isNotNull() |\n                                         f.col('OR').isNotNull() |\n                                         f.col('Ward').isNotNull())\nadt_adults_wide = adt_adults_wide.select('C19_HAR_ID').distinct()\n\nadults_in_time = adt_adults_wide.join(adults_in_time, on='C19_HAR_ID', how='left')\nhar_ids = adults_in_time.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime')\n\n## Explode between admission and discharge to get all hourly timestamps\nhar_ids_hours = har_ids.withColumn('txnDt', f.explode(f.expr('sequence(admission_datetime, discharge_datetime, interval 1 hour)')))\nhar_ids_hours = har_ids_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\nhar_ids_hours = har_ids_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\nhar_ids_hours = har_ids_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour').orderBy('C19_HAR_ID', 'txnDt')\n\n\n\nUse RCLIF_resp_support table to identify instances of invasive or non-invasive mechanical ventilation. If no invasive or non-invasive mechanical ventilation, use RCLIF_resp_support, RCLIF_labs, and RCLIF_vitals to calculate PaO2/FiO2 or Sp02.\n\n\n\nInner join RCLIF_resp_support to adults hospitalized in time period, identified in previous code block.\nFilter out invalid FiO2 measurements\n\nN removed: 3,440 observations; 376 unique encounters\n\nRemove instances of CPAP\n\nCPAP defined as device_name == “NIPPV” and without an FiO2 measurement, or when mode_name == “CPAP”\nN removed: 357,851 observations; 128 unique encounters\n\nIf device_name was missing, fill in based on the following logic:\n\nIf fio2 ==.21 and lpm, peep, and set_volume are null, then device_name == Room Air\nIf fio2 ==.21, lpm==0, and peep and set_volume are null, then device_name == Room Air\nIf fio2, peep, and set_volume are null and lpm &lt;=20 then device_name == Nasal Cannula\nIf fio2, peep, and set_volume are null and lpm &gt;20 then device_name == High Flow NC\nIf fio2 and lpm are null and set_volumne is not null, then device_name == Vent\n\nIf device_name was Nasal Cannula but lpm &gt; 20, then device_name== High Flow NC\nIf fio2 is missing and device_name == Room Air, then fio2== 0.21\nIf fio2 is missing and device_name == Nasal Cannula, then fio2 was calculated as: ( 0.24 + (0.04 * lpm) )\n\n######### Get worst FiO2\n\n## Read in respiratory support table\nresp_full = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_resp_support.csv')\nresp_full = resp_full.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\nresp_full = resp_full.withColumn(\"fio2\",resp_full.fio2.cast('double'))\nresp_full = resp_full.withColumn(\"lpm\",resp_full.lpm.cast('double'))\nresp_full = resp_full.withColumn(\"peep\",resp_full.peep.cast('double'))\nresp_full = resp_full.withColumn(\"set_volume\",resp_full.peep.cast('double'))\n\n\n## Filter to only adults in time frame\nresp_full = har_ids.join(resp_full, on='C19_HAR_ID', how='inner')\n##Filter to only variables we need\nresp_full = resp_full.select('C19_PATIENT_ID', 'C19_HAR_ID', 'recorded_time', 'device_name', 'mode_name', 'mode_category','lpm', 'fio2', 'peep', 'set_volume')\n\n## Filter for valid values or Null\nresp_full = resp_full.filter((((f.col('fio2')&gt;=0.21) &\n                              (f.col('fio2')&lt;=1)) |\n                              (f.col('fio2').isNull())))\nresp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n\n## Filter out people on NIPPV without a FiO2 measurement--CPAP\nnippv_test = resp_full.filter(f.col('device_name')==\"NIPPV\")\nnippv_test.summary().show()\n\nresp_full = resp_full.filter(~((f.col('device_name')=='NIPPV') &\n                              (f.col('fio2').isNull())&\n                              (f.col('lpm').isNull()) &\n                              (f.col('peep').isNull())))\n\nresp_full = resp_full.filter(~f.col('mode_name').rlike(r'CPAP'))\nresp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n\n## Replace \"NA\" strings with actual Nulls\nresp_full.select('device_name').distinct().show()\nresp_full = resp_full.withColumn('device_name', f.when(~f.col('device_name').rlike(r'NA'), f.col('device_name')))\n\n## Try to fill in some of the null device names based on other values\nresp_full = resp_full.withColumn('device_name_2', f.expr(\n        \"\"\"\n        CASE\n        WHEN device_name IS NOT NULL THEN device_name\n        WHEN device_name IS NULL AND fio2 ==.21 AND lpm IS NULL AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm ==0 AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm &lt;=20 AND peep IS NULL AND set_volume IS NULL THEN 'Nasal Cannula'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm &gt;20 AND peep IS NULL AND set_volume IS NULL THEN 'High Flow NC'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm IS NULL AND set_volume IS NOT NULL THEN 'Vent'\n        WHEN device_name == \"Nasal Cannula\" AND fio2 IS NULL AND lpm &gt;20 THEN 'High Flow NC'\n        ELSE NULL\n        END\n        \"\"\"\n))\n\n## Try to fill in FiO2 based on LPM for nasal cannula\nresp_full = resp_full.withColumn('fio2_combined', f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2 IS NOT NULL THEN fio2\n        WHEN fio2 IS NULL AND device_name_2 == 'Room Air' THEN .21\n        WHEN fio2 IS NULL AND device_name_2 == 'Nasal Cannula' THEN ( 0.24 + (0.04 * lpm) )\n        ELSE NULL\n        END\n        \"\"\"\n))\n\n\n\nGoal: For each one hour interval, identify highest level of respiratory support and maximum FiO2.\n\nExtract the measurement hour and measurement date from the recorded_time\nRank respiratory support devices\nGroup by C19_HAR_ID, measurement date, and measurement hour, then take the minimum of device_rank (highest level of support) and maximum of fio2\n\n\n## Extract hour and date for blocking\nresp_full = resp_full.select('C19_HAR_ID', 'device_name_2', 'recorded_time', 'fio2_combined')\nresp_full = resp_full.withColumn('meas_hour', f.hour(f.col('recorded_time')))\nresp_full = resp_full.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n\nfio2 = resp_full.select('C19_HAR_ID', 'device_name_2', 'meas_date', 'meas_hour', 'fio2_combined').distinct()\n\n## Need to rank devices to get max in hour\nfio2 = fio2.withColumn(\"device_rank\", f.expr(\n        \"\"\"\n        CASE\n        WHEN device_name_2 == 'Vent' THEN 1\n        WHEN device_name_2 == 'NIPPV' THEN 2\n        WHEN device_name_2 == 'High Flow NC' THEN 3\n        WHEN device_name_2 == 'Face Mask' THEN 4 \n        WHEN device_name_2 == 'Trach Collar' THEN 5\n        WHEN device_name_2 == 'Nasal Cannula' THEN 6 \n        WHEN device_name_2 == 'Other' THEN 7\n        WHEN device_name_2 == 'Room Air' THEN 8\n        WHEN device_name_2 IS NULL THEN NULL\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\n## Group by person, device, measurement date and measurement hour; get max FiO2 and LPM within each hour\ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nfio2 = fio2.groupBy(group_cols) \\\n            .agg((f.max('fio2_combined').alias(\"fio2_combined\")),\n                  (f.min('device_rank').alias(\"device_rank\"))).orderBy(group_cols)\nfio2 = fio2.withColumn(\"device_name\", f.expr(\n        \"\"\"\n        CASE\n        WHEN device_rank == 1 THEN 'Vent'\n        WHEN device_rank == 2 THEN 'NIPPV' \n        WHEN device_rank == 3 THEN 'High Flow NC'\n        WHEN device_rank == 4 THEN 'Face Mask' \n        WHEN device_rank == 5 THEN 'Trach Collar'\n        WHEN device_rank == 6 THEN 'Nasal Cannula'\n        WHEN device_rank == 7 THEN 'Other'\n        WHEN device_rank == 8 THEN 'Room Air'\n        WHEN device_rank IS NULL THEN NULL\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n    \nNow, you have a dataset with one observation per C19_HAR_ID per hour, with the values representing their worst respiratory state in that hour, but only for the hours that have a recorded value. For example, if someone is on a Nasal Cannula with a constant setting for many hours, only the hour that they were started on that device may have an entry. Therefore, we need to carry forward values until another value is recorded, indicating a change in respiratory support, discharge, or death. This will allow us to merge in the minimum PaO2 values per hour using the RCLIF_labs table (see section #) and ensure the PaO2 and FiO2 were measured within the same hour when calculating the PaO2/FiO2 ratio.\nTo do this, we: 1. Left join back to hourly blocked dataset created in previous code chunk 2. Carry forward values until a change or until discharge\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nfio2_hours = har_ids_hours.join(fio2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Carry forward device name until another device is recorded or the end of the measurement time window\nfio2_hours = fio2_hours.withColumn('device_filled', \n                                       f.coalesce(f.col('device_name'), \n                                                  f.last('device_name', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\nfio2_hours = fio2_hours.withColumn('device_filled', \n                                       f.when(~f.col('device_filled').rlike(r'NULL'), f.col('device_filled')))\n\n## Carry forward FiO2 measurement name until another device is recorded or the end of the measurement time window\nfio2_hours = fio2_hours.withColumn(\"fio2_combined\",fio2_hours.fio2_combined.cast('double'))\n\nfio2_hours = fio2_hours.withColumn('fio2_filled', \n                                       f.when((f.col('fio2_combined').isNotNull()), f.col('fio2_combined')))\nfio2_hours = fio2_hours.withColumn('fio2_filled', \n                                       f.coalesce(f.col('fio2_combined'), \n                                                  f.last('fio2_combined', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID', 'device_filled') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\n\nfio2_filled = fio2_hours.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour',\n                                  'device_filled','fio2_filled').distinct()\n                                  \nNow, we need to bring in PaO2 values using the RCLIF_labs table to calculate PaO2/FiO2 ratio. We will conduct a similar process as above to create an hourly blocked dataset of the minimum PaO2 per hour.\n\nInner join RCLIF_labs to adults hospitalized in time period\nFilter to just PaO2 labs\nGroup by C19_HAR_ID, measurement date, and measurement hour, then take the minimum lab_value\nLeft join back to hourly blocked dataset\nCarry forward values for only 4 hours\n\n\n# Now need PaO2\nlabs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\n\n### Selecting only variables we need\nlabs = labs.select('C19_HAR_ID', 'lab_result_time','lab_name', 'lab_value')\n\n### Filtering to only adults in time period\nlabs = labs.join(har_ids, on='C19_HAR_ID', how='inner')\n\n### Filtering to only PaO2, formatting values\nlabs = labs.filter(f.col(\"lab_name\")==\"pao2\")\nlabs = labs.withColumn(\"lab_value\",labs.lab_value.cast('double'))\nlabs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n\n# Get min PaO2 per hour\nlabs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\nlabs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\npao2 = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\npao2 = pao2.groupBy(group_cols) \\\n           .pivot(\"lab_name\") \\\n           .agg(f.min('lab_value').alias(\"min\")).orderBy(group_cols)\n\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\npao2_hours = har_ids_hours.join(pao2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Carry forward PaO2 until next measurement or end of window, maximum 4 hours\n\n### Get time of most recent PaO2\npao2_hours = pao2_hours.withColumn('last_measure', f.when(f.col('pao2').isNotNull(), f.col('txnDt')))\npao2_hours = pao2_hours.withColumn('last_measure', f.coalesce(f.col('last_measure'), \n                                                                  f.last('last_measure', True)\\\n                                                                  .over(Window.partitionBy('C19_HAR_ID')\\\n                                                                        .orderBy('txnDt')), f.lit('NULL')))\n\npao2_hours = pao2_hours.withColumn('last_measure',f.to_timestamp('last_measure','yyyy-MM-dd HH:mm:ss'))\npao2_hours = pao2_hours.withColumn('txnDt',f.to_timestamp('txnDt','yyyy-MM-dd HH:mm:ss'))\n\n### Calculate time difference between the hour we're trying to fill and the most recent PaO2, filter to only 3 additional hrs (4 total)\npao2_hours = pao2_hours.withColumn(\"hour_diff\", \n                                       (f.col(\"txnDt\").cast(\"long\")-f.col(\"last_measure\").cast(\"long\"))/(60*60))\npao2_hours_2 = pao2_hours.filter((f.col('hour_diff')&gt;=0)&(f.col('hour_diff')&lt;=3))\n\n### Fill PaO2 forward\npao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.when(f.col('pao2').isNotNull(), f.col('pao2')))\npao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.coalesce(f.col('pao2'), \n                                                                 f.last('pao2', True)\\\n                                                                 .over(Window.partitionBy('C19_HAR_ID', \n                                                                                          'last_measure')\\\n                                                                       .orderBy('txnDt')), f.lit('NULL')))\n\npao2_filled = pao2_hours_2.select('C19_HAR_ID','meas_date', 'meas_hour', 'pao2_filled')\nNext, we repeat the process with the RCLIF_vitals table to obtain SpO2 measurements. In the absence of a PaO2/FiO2 ratio, we use SpO2/FiO2 ratio to determine cohort inclusion.\n\n# Now need spO2\nvitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals.parquet\")\n\n### Filtering to only adults in time period\nvitals = vitals.join(har_ids, on='C19_HAR_ID', how='inner')\n\n### Formatting values\nvitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\nvitals = vitals.withColumn(\"vital_value\",vitals.vital_value.cast('double'))\n\n### Selecting variables we need\nvitals = vitals.select('C19_HAR_ID', 'measured_time','vital_name', 'vital_value')\nvitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\nvitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\n\n### Filtering to only SpO2, valid values\nspo2 = vitals.filter(f.col(\"vital_name\")==\"spO2\")\nspo2 = spo2.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'vital_name', 'vital_value')\nspo2 = spo2.filter(f.col('vital_value')&gt;60)\nspo2 = spo2.filter(f.col('vital_value')&lt;=100)\n\n# Get min SpO2 per hour\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\nspo2 = spo2.groupBy(group_cols) \\\n           .pivot(\"vital_name\") \\\n           .agg(f.min('vital_value').alias(\"min\"))\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nspo2_hours = har_ids_hours.join(spo2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Cary forward SpO2\nspo2_hours = spo2_hours.withColumn('spO2_filled', \n                                           f.when(f.col('spO2').isNotNull(), f.col('spO2')))\nspo2_hours = spo2_hours.withColumn('spO2_filled', \n                                           f.coalesce(f.col('spO2'), \n                                                      f.last('spO2', True)\\\n                                                      .over(Window.partitionBy('C19_HAR_ID', 'last_measure')\\\n                                                            .orderBy('txnDt')), f.lit('NULL')))\n\nspO2_filled = spo2_hours.select('C19_HAR_ID','meas_date', 'meas_hour', 'spO2_filled')\n\nNow that we have hourly blocked respiratory support, FiO2, PaO2, and SpO2, we can full join these tables to determine the first hour a patient met the respiratory inclusion criteria (the minimum recorded time of either FiO2/PaO2 &lt; 200, SpO2/FiO2 &lt;179, non-invasive ventilation, or invasive ventilation). I also write this hourly blocked dataset for later use in the 48-hour analytic dataset.\n\n# Merge FiO2, PaO2, spO2 to get FiO2/PaO2\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\ndf = fio2_filled.join(pao2_filled, on=group_cols, how='full')\ndf = df.join(spO2_filled, on=group_cols, how='full').orderBy(group_cols)\n\ndf = df.withColumn(\"fio2_filled\",df.fio2_filled.cast('double'))\ndf = df.withColumn(\"pao2_filled\",df.pao2_filled.cast('double'))\ndf = df.withColumn(\"spO2_filled\",df.spO2_filled.cast('double'))\n\n# Get first time on oxygen support & P/F &lt;200\ndf = df.withColumn(\"p_f\", f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2_filled IS NOT NULL AND pao2_filled IS NOT NULL THEN ( pao2_filled / fio2_filled )\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\ndf = df.withColumn(\"s_f\", f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2_filled IS NOT NULL AND spO2_filled IS NOT NULL THEN ( spO2_filled / fio2_filled )\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\ndf = df.distinct()\ndf.write.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\", mode=\"overwrite\")\n\n## Get first time somone on oxygen therapy with PaO2/FiO2 &lt; 200 (S/F &lt; 179 if no P/F measured) and invasive and non-invasive ventilation\n\ndf = df.filter((((f.col(\"p_f\")&lt;200))|\n                (f.col(\"s_f\")&lt;179) |\n                (f.col('device_filled')=='Vent') | \n                (f.col('device_filled')=='NIPPV')))\n\n\ndf = df.select(\"C19_HAR_ID\", \"txnDt\", \"device_filled\",\"pao2_filled\",\"fio2_filled\", \"spO2_filled\", \"p_f\", \"s_f\")\n\nw1 = Window.partitionBy(\"C19_HAR_ID\").orderBy('txnDt')\n\ndf_first_with_time = df.withColumn(\"row\",f.row_number().over(w1)) \\\n             .filter(f.col(\"row\") == 1).drop(\"row\")\n\ndf_first_with_time = df_first_with_time.select(\"C19_HAR_ID\", \"txnDt\").withColumnRenamed(\"txnDt\", \"recorded_time\")\n\nresp_support = df_first_with_time.groupBy(\"C19_HAR_ID\").agg(f.min(\"recorded_time\").alias(\"resp_life_support_start\")).distinct()\n\n\n\n\nUsing the RCLIF_medication_admin_continuous table, we identify the first instance a patient is started on a vasopressor. We inner join the adults hospitalized in the time period to the RCLIF_medication_admin_continuous table, then filter to only vasopressor medications, then take the first time per patient per encounter to use as the index time.\n# Now pressors\ndf_meds = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_meds_admin_conti.csv')\ndf_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_time','yyyy-MM-dd HH:mm:ss'))\n\n# Filter to pressor medications\n\npressors = df_meds.filter(f.col('med_category')=='vasoactives')\npressors = pressors.select(\"C19_HAR_ID\", \"admin_time\")\n\n# Get first time someone is on a pressor\npressors = pressors.groupBy(\"C19_HAR_ID\").agg(f.min(\"admin_time\").alias(\"pressor_life_support_start\"))\npressors = pressors.join(har_ids, on='C19_HAR_ID', how='inner').distinct()\n\n\n\nThe start of life support will be the index time for constructing the 48 hour analytic dataset (41 hours before, the index hour, and 6 hours after). To do that, we take the minimum between the time respiratory criteria was met and the time vasopressors were started. Now we have a cohort identified with the first hour block life support was initiated (index time).\nWe also calculate the start of the window time and end of the window time, and “explode” between these times to create the base 48 hour blocked dataset for the cohort that subsequent data will be left joined to.\n\n# Get first time on life support\n\ndf = pressors.join(resp_support, on='C19_HAR_ID', how='full')\ndf = df.withColumn(\"life_support_start\", f.least(f.col('pressor_life_support_start'),\n                                                 f.col('resp_life_support_start')))\ndf = df.filter(f.col('life_support_start').isNotNull())\n\ndf = df.withColumn('window_start', (f.col('life_support_start')-f.expr(\"INTERVAL 41 HOURS\")))\ndf = df.withColumn('window_end', (f.col('life_support_start')+f.expr(\"INTERVAL 6 HOURS\")))\ndf = df.select(\"C19_HAR_ID\", \"life_support_start\", \"window_start\", \"window_end\")\n\n## Re-join age at admission and disposition, admission & discharge dttm, and zip\ndf = df.join(limited_ids, on='C19_HAR_ID', how='left')\ndf = df.join(demo_disp, on='C19_HAR_ID', how='left')\ndf = df.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \"age_at_admission\",\n               \"disposition\")\n\ndf.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\", mode=\"overwrite\")\n\n## Explode between window to get all hourly timestamps\ncohort_hours = df.withColumn('txnDt', \n                                   f.explode(f.expr('sequence(window_start, window_end, interval 1 hour)')))\ncohort_hours = cohort_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\ncohort_hours = cohort_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\ncohort_hours = cohort_hours.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \n                                   \"age_at_admission\", \"disposition\", \"meas_date\", \"meas_hour\").distinct()\n\ncohort_hours.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\", mode=\"overwrite\")"
  },
  {
    "objectID": "posts/sipa_blog/SIPA_blog_post.html#cohort-identification",
    "href": "posts/sipa_blog/SIPA_blog_post.html#cohort-identification",
    "title": "Constructing an Analytic Dataset from CLIF",
    "section": "",
    "text": "Cohort Definition: Data from UCMC COVID Datamart. Adult patients (18+) admitted January 1 2020 – March 31, 2022 and who recieved life support. Life support was defined as:\n\nReceived vasoactive medications for shock, or\nReceived invasive or non-invasive mechanical ventilation, or\nReceived oxygen therapy with PaO2/FiO2 &lt; 200 (S/F &lt; 179 if no P/F measured)\n\nPatients were excluded in they were discharged from the ED (including to hospice) and if they received &lt; 6 hours of life support.\nCLIF tables used:\n\nRCLIF_limited_identifers (admission date, discharge date)\nRCLIF_encounter_demographics_dispo (age at admission, disposition)\nRCLIF_adt (location in hospital)\nRCLIF_resp_support (respiratory support device, FiO2)\nRCLIF_labs (PaO2)\nRCLIF_vitals (SpO2)\nRCLIF_medication_adm_continuous (vasopressors)\n\n\n\n\nUse RCLIF_limited_identifers to identify admissions in the time period of interest (variable: admission_dttm)\nUse RCLIF_encounter_demographics_dispo to filter for only those 18+ at time of admission (variable: age_at_admission)\nUse the ADT table to filter out those only seen in the ER (variable: dept_name)\n“Explode” the data between admission_dttm and discharge_dttm to get hourly blocks for the duration of admission\n\n\n######## Load in limited IDs for admission dates in time period\nlimited_ids = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_limited_identifers.csv\")\nlimited_ids = limited_ids.withColumn('admission_datetime',f.to_timestamp('admission_date','yyyy-MM-dd HH:mm:ss'))\nlimited_ids = limited_ids.withColumn('discharge_datetime',f.to_timestamp('discharge_date','yyyy-MM-dd HH:mm:ss'))\nlimited_ids = limited_ids.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime', 'zip_code').distinct()\n\n# Filter to time period\nlimited_ids = limited_ids.filter(((f.col('admission_datetime')&gt;='2020-03-01') & \n                   (f.col('admission_datetime')&lt;='2022-03-31')))\nlimited_ids = limited_ids.filter((f.col('discharge_datetime')&gt;=f.col('admission_datetime')))\nlimited_ids = limited_ids.filter(f.col('discharge_datetime').isNotNull())\nlimited_ids = limited_ids.filter(f.col('admission_datetime').isNotNull())\n\n\n######## Load in encounters for age and discharge disposition\ndemo_disp = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_encounter_demographics_dispo.csv\")\ndemo_disp = demo_disp.withColumn(\"age_at_admission\",demo_disp.age_at_admission.cast('double'))\ndemo_disp = demo_disp.select('C19_PATIENT_ID', 'C19_HAR_ID', 'age_at_admission', 'disposition')\n\n# Filter to adults only\ndemo_disp = demo_disp.filter(f.col('age_at_admission')&gt;=18)\n\nadults_in_time = limited_ids.join(demo_disp, on='C19_HAR_ID', how='inner')\n\n# Exclude people only in ER\nadt = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_adt.csv\")\n\nadult_ids = adults_in_time.select('C19_HAR_ID')\nadt_adults = adult_ids.join(adt, on='C19_HAR_ID', how='inner')\nadt_adults = adt_adults.select('C19_HAR_ID', 'dept_name').distinct()\nadt_adults = adt_adults.withColumn('count', f.lit(1))\nadt_adults_wide = adt_adults.groupBy('C19_HAR_ID').pivot('dept_name').agg(f.sum('count'))\nadt_adults_wide = adt_adults_wide.filter(f.col('ICU').isNotNull() |\n                                         f.col('NA').isNotNull() |\n                                         f.col('OR').isNotNull() |\n                                         f.col('Ward').isNotNull())\nadt_adults_wide = adt_adults_wide.select('C19_HAR_ID').distinct()\n\nadults_in_time = adt_adults_wide.join(adults_in_time, on='C19_HAR_ID', how='left')\nhar_ids = adults_in_time.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime')\n\n## Explode between admission and discharge to get all hourly timestamps\nhar_ids_hours = har_ids.withColumn('txnDt', f.explode(f.expr('sequence(admission_datetime, discharge_datetime, interval 1 hour)')))\nhar_ids_hours = har_ids_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\nhar_ids_hours = har_ids_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\nhar_ids_hours = har_ids_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour').orderBy('C19_HAR_ID', 'txnDt')\n\n\n\nUse RCLIF_resp_support table to identify instances of invasive or non-invasive mechanical ventilation. If no invasive or non-invasive mechanical ventilation, use RCLIF_resp_support, RCLIF_labs, and RCLIF_vitals to calculate PaO2/FiO2 or Sp02.\n\n\n\nInner join RCLIF_resp_support to adults hospitalized in time period, identified in previous code block.\nFilter out invalid FiO2 measurements\n\nN removed: 3,440 observations; 376 unique encounters\n\nRemove instances of CPAP\n\nCPAP defined as device_name == “NIPPV” and without an FiO2 measurement, or when mode_name == “CPAP”\nN removed: 357,851 observations; 128 unique encounters\n\nIf device_name was missing, fill in based on the following logic:\n\nIf fio2 ==.21 and lpm, peep, and set_volume are null, then device_name == Room Air\nIf fio2 ==.21, lpm==0, and peep and set_volume are null, then device_name == Room Air\nIf fio2, peep, and set_volume are null and lpm &lt;=20 then device_name == Nasal Cannula\nIf fio2, peep, and set_volume are null and lpm &gt;20 then device_name == High Flow NC\nIf fio2 and lpm are null and set_volumne is not null, then device_name == Vent\n\nIf device_name was Nasal Cannula but lpm &gt; 20, then device_name== High Flow NC\nIf fio2 is missing and device_name == Room Air, then fio2== 0.21\nIf fio2 is missing and device_name == Nasal Cannula, then fio2 was calculated as: ( 0.24 + (0.04 * lpm) )\n\n######### Get worst FiO2\n\n## Read in respiratory support table\nresp_full = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_resp_support.csv')\nresp_full = resp_full.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\nresp_full = resp_full.withColumn(\"fio2\",resp_full.fio2.cast('double'))\nresp_full = resp_full.withColumn(\"lpm\",resp_full.lpm.cast('double'))\nresp_full = resp_full.withColumn(\"peep\",resp_full.peep.cast('double'))\nresp_full = resp_full.withColumn(\"set_volume\",resp_full.peep.cast('double'))\n\n\n## Filter to only adults in time frame\nresp_full = har_ids.join(resp_full, on='C19_HAR_ID', how='inner')\n##Filter to only variables we need\nresp_full = resp_full.select('C19_PATIENT_ID', 'C19_HAR_ID', 'recorded_time', 'device_name', 'mode_name', 'mode_category','lpm', 'fio2', 'peep', 'set_volume')\n\n## Filter for valid values or Null\nresp_full = resp_full.filter((((f.col('fio2')&gt;=0.21) &\n                              (f.col('fio2')&lt;=1)) |\n                              (f.col('fio2').isNull())))\nresp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n\n## Filter out people on NIPPV without a FiO2 measurement--CPAP\nnippv_test = resp_full.filter(f.col('device_name')==\"NIPPV\")\nnippv_test.summary().show()\n\nresp_full = resp_full.filter(~((f.col('device_name')=='NIPPV') &\n                              (f.col('fio2').isNull())&\n                              (f.col('lpm').isNull()) &\n                              (f.col('peep').isNull())))\n\nresp_full = resp_full.filter(~f.col('mode_name').rlike(r'CPAP'))\nresp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n\n## Replace \"NA\" strings with actual Nulls\nresp_full.select('device_name').distinct().show()\nresp_full = resp_full.withColumn('device_name', f.when(~f.col('device_name').rlike(r'NA'), f.col('device_name')))\n\n## Try to fill in some of the null device names based on other values\nresp_full = resp_full.withColumn('device_name_2', f.expr(\n        \"\"\"\n        CASE\n        WHEN device_name IS NOT NULL THEN device_name\n        WHEN device_name IS NULL AND fio2 ==.21 AND lpm IS NULL AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm ==0 AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm &lt;=20 AND peep IS NULL AND set_volume IS NULL THEN 'Nasal Cannula'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm &gt;20 AND peep IS NULL AND set_volume IS NULL THEN 'High Flow NC'\n        WHEN device_name IS NULL AND fio2 IS NULL AND lpm IS NULL AND set_volume IS NOT NULL THEN 'Vent'\n        WHEN device_name == \"Nasal Cannula\" AND fio2 IS NULL AND lpm &gt;20 THEN 'High Flow NC'\n        ELSE NULL\n        END\n        \"\"\"\n))\n\n## Try to fill in FiO2 based on LPM for nasal cannula\nresp_full = resp_full.withColumn('fio2_combined', f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2 IS NOT NULL THEN fio2\n        WHEN fio2 IS NULL AND device_name_2 == 'Room Air' THEN .21\n        WHEN fio2 IS NULL AND device_name_2 == 'Nasal Cannula' THEN ( 0.24 + (0.04 * lpm) )\n        ELSE NULL\n        END\n        \"\"\"\n))\n\n\n\nGoal: For each one hour interval, identify highest level of respiratory support and maximum FiO2.\n\nExtract the measurement hour and measurement date from the recorded_time\nRank respiratory support devices\nGroup by C19_HAR_ID, measurement date, and measurement hour, then take the minimum of device_rank (highest level of support) and maximum of fio2\n\n\n## Extract hour and date for blocking\nresp_full = resp_full.select('C19_HAR_ID', 'device_name_2', 'recorded_time', 'fio2_combined')\nresp_full = resp_full.withColumn('meas_hour', f.hour(f.col('recorded_time')))\nresp_full = resp_full.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n\nfio2 = resp_full.select('C19_HAR_ID', 'device_name_2', 'meas_date', 'meas_hour', 'fio2_combined').distinct()\n\n## Need to rank devices to get max in hour\nfio2 = fio2.withColumn(\"device_rank\", f.expr(\n        \"\"\"\n        CASE\n        WHEN device_name_2 == 'Vent' THEN 1\n        WHEN device_name_2 == 'NIPPV' THEN 2\n        WHEN device_name_2 == 'High Flow NC' THEN 3\n        WHEN device_name_2 == 'Face Mask' THEN 4 \n        WHEN device_name_2 == 'Trach Collar' THEN 5\n        WHEN device_name_2 == 'Nasal Cannula' THEN 6 \n        WHEN device_name_2 == 'Other' THEN 7\n        WHEN device_name_2 == 'Room Air' THEN 8\n        WHEN device_name_2 IS NULL THEN NULL\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\n## Group by person, device, measurement date and measurement hour; get max FiO2 and LPM within each hour\ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nfio2 = fio2.groupBy(group_cols) \\\n            .agg((f.max('fio2_combined').alias(\"fio2_combined\")),\n                  (f.min('device_rank').alias(\"device_rank\"))).orderBy(group_cols)\nfio2 = fio2.withColumn(\"device_name\", f.expr(\n        \"\"\"\n        CASE\n        WHEN device_rank == 1 THEN 'Vent'\n        WHEN device_rank == 2 THEN 'NIPPV' \n        WHEN device_rank == 3 THEN 'High Flow NC'\n        WHEN device_rank == 4 THEN 'Face Mask' \n        WHEN device_rank == 5 THEN 'Trach Collar'\n        WHEN device_rank == 6 THEN 'Nasal Cannula'\n        WHEN device_rank == 7 THEN 'Other'\n        WHEN device_rank == 8 THEN 'Room Air'\n        WHEN device_rank IS NULL THEN NULL\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n    \nNow, you have a dataset with one observation per C19_HAR_ID per hour, with the values representing their worst respiratory state in that hour, but only for the hours that have a recorded value. For example, if someone is on a Nasal Cannula with a constant setting for many hours, only the hour that they were started on that device may have an entry. Therefore, we need to carry forward values until another value is recorded, indicating a change in respiratory support, discharge, or death. This will allow us to merge in the minimum PaO2 values per hour using the RCLIF_labs table (see section #) and ensure the PaO2 and FiO2 were measured within the same hour when calculating the PaO2/FiO2 ratio.\nTo do this, we: 1. Left join back to hourly blocked dataset created in previous code chunk 2. Carry forward values until a change or until discharge\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nfio2_hours = har_ids_hours.join(fio2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Carry forward device name until another device is recorded or the end of the measurement time window\nfio2_hours = fio2_hours.withColumn('device_filled', \n                                       f.coalesce(f.col('device_name'), \n                                                  f.last('device_name', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\nfio2_hours = fio2_hours.withColumn('device_filled', \n                                       f.when(~f.col('device_filled').rlike(r'NULL'), f.col('device_filled')))\n\n## Carry forward FiO2 measurement name until another device is recorded or the end of the measurement time window\nfio2_hours = fio2_hours.withColumn(\"fio2_combined\",fio2_hours.fio2_combined.cast('double'))\n\nfio2_hours = fio2_hours.withColumn('fio2_filled', \n                                       f.when((f.col('fio2_combined').isNotNull()), f.col('fio2_combined')))\nfio2_hours = fio2_hours.withColumn('fio2_filled', \n                                       f.coalesce(f.col('fio2_combined'), \n                                                  f.last('fio2_combined', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID', 'device_filled') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\n\nfio2_filled = fio2_hours.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour',\n                                  'device_filled','fio2_filled').distinct()\n                                  \nNow, we need to bring in PaO2 values using the RCLIF_labs table to calculate PaO2/FiO2 ratio. We will conduct a similar process as above to create an hourly blocked dataset of the minimum PaO2 per hour.\n\nInner join RCLIF_labs to adults hospitalized in time period\nFilter to just PaO2 labs\nGroup by C19_HAR_ID, measurement date, and measurement hour, then take the minimum lab_value\nLeft join back to hourly blocked dataset\nCarry forward values for only 4 hours\n\n\n# Now need PaO2\nlabs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\n\n### Selecting only variables we need\nlabs = labs.select('C19_HAR_ID', 'lab_result_time','lab_name', 'lab_value')\n\n### Filtering to only adults in time period\nlabs = labs.join(har_ids, on='C19_HAR_ID', how='inner')\n\n### Filtering to only PaO2, formatting values\nlabs = labs.filter(f.col(\"lab_name\")==\"pao2\")\nlabs = labs.withColumn(\"lab_value\",labs.lab_value.cast('double'))\nlabs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n\n# Get min PaO2 per hour\nlabs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\nlabs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\npao2 = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\npao2 = pao2.groupBy(group_cols) \\\n           .pivot(\"lab_name\") \\\n           .agg(f.min('lab_value').alias(\"min\")).orderBy(group_cols)\n\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\npao2_hours = har_ids_hours.join(pao2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Carry forward PaO2 until next measurement or end of window, maximum 4 hours\n\n### Get time of most recent PaO2\npao2_hours = pao2_hours.withColumn('last_measure', f.when(f.col('pao2').isNotNull(), f.col('txnDt')))\npao2_hours = pao2_hours.withColumn('last_measure', f.coalesce(f.col('last_measure'), \n                                                                  f.last('last_measure', True)\\\n                                                                  .over(Window.partitionBy('C19_HAR_ID')\\\n                                                                        .orderBy('txnDt')), f.lit('NULL')))\n\npao2_hours = pao2_hours.withColumn('last_measure',f.to_timestamp('last_measure','yyyy-MM-dd HH:mm:ss'))\npao2_hours = pao2_hours.withColumn('txnDt',f.to_timestamp('txnDt','yyyy-MM-dd HH:mm:ss'))\n\n### Calculate time difference between the hour we're trying to fill and the most recent PaO2, filter to only 3 additional hrs (4 total)\npao2_hours = pao2_hours.withColumn(\"hour_diff\", \n                                       (f.col(\"txnDt\").cast(\"long\")-f.col(\"last_measure\").cast(\"long\"))/(60*60))\npao2_hours_2 = pao2_hours.filter((f.col('hour_diff')&gt;=0)&(f.col('hour_diff')&lt;=3))\n\n### Fill PaO2 forward\npao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.when(f.col('pao2').isNotNull(), f.col('pao2')))\npao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.coalesce(f.col('pao2'), \n                                                                 f.last('pao2', True)\\\n                                                                 .over(Window.partitionBy('C19_HAR_ID', \n                                                                                          'last_measure')\\\n                                                                       .orderBy('txnDt')), f.lit('NULL')))\n\npao2_filled = pao2_hours_2.select('C19_HAR_ID','meas_date', 'meas_hour', 'pao2_filled')\nNext, we repeat the process with the RCLIF_vitals table to obtain SpO2 measurements. In the absence of a PaO2/FiO2 ratio, we use SpO2/FiO2 ratio to determine cohort inclusion.\n\n# Now need spO2\nvitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals.parquet\")\n\n### Filtering to only adults in time period\nvitals = vitals.join(har_ids, on='C19_HAR_ID', how='inner')\n\n### Formatting values\nvitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\nvitals = vitals.withColumn(\"vital_value\",vitals.vital_value.cast('double'))\n\n### Selecting variables we need\nvitals = vitals.select('C19_HAR_ID', 'measured_time','vital_name', 'vital_value')\nvitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\nvitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\n\n### Filtering to only SpO2, valid values\nspo2 = vitals.filter(f.col(\"vital_name\")==\"spO2\")\nspo2 = spo2.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'vital_name', 'vital_value')\nspo2 = spo2.filter(f.col('vital_value')&gt;60)\nspo2 = spo2.filter(f.col('vital_value')&lt;=100)\n\n# Get min SpO2 per hour\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\nspo2 = spo2.groupBy(group_cols) \\\n           .pivot(\"vital_name\") \\\n           .agg(f.min('vital_value').alias(\"min\"))\n\n## Merge back to hourly blocked cohort table \ngroup_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\nspo2_hours = har_ids_hours.join(spo2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n\n\n## Cary forward SpO2\nspo2_hours = spo2_hours.withColumn('spO2_filled', \n                                           f.when(f.col('spO2').isNotNull(), f.col('spO2')))\nspo2_hours = spo2_hours.withColumn('spO2_filled', \n                                           f.coalesce(f.col('spO2'), \n                                                      f.last('spO2', True)\\\n                                                      .over(Window.partitionBy('C19_HAR_ID', 'last_measure')\\\n                                                            .orderBy('txnDt')), f.lit('NULL')))\n\nspO2_filled = spo2_hours.select('C19_HAR_ID','meas_date', 'meas_hour', 'spO2_filled')\n\nNow that we have hourly blocked respiratory support, FiO2, PaO2, and SpO2, we can full join these tables to determine the first hour a patient met the respiratory inclusion criteria (the minimum recorded time of either FiO2/PaO2 &lt; 200, SpO2/FiO2 &lt;179, non-invasive ventilation, or invasive ventilation). I also write this hourly blocked dataset for later use in the 48-hour analytic dataset.\n\n# Merge FiO2, PaO2, spO2 to get FiO2/PaO2\n\ngroup_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\ndf = fio2_filled.join(pao2_filled, on=group_cols, how='full')\ndf = df.join(spO2_filled, on=group_cols, how='full').orderBy(group_cols)\n\ndf = df.withColumn(\"fio2_filled\",df.fio2_filled.cast('double'))\ndf = df.withColumn(\"pao2_filled\",df.pao2_filled.cast('double'))\ndf = df.withColumn(\"spO2_filled\",df.spO2_filled.cast('double'))\n\n# Get first time on oxygen support & P/F &lt;200\ndf = df.withColumn(\"p_f\", f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2_filled IS NOT NULL AND pao2_filled IS NOT NULL THEN ( pao2_filled / fio2_filled )\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\ndf = df.withColumn(\"s_f\", f.expr(\n        \"\"\"\n        CASE\n        WHEN fio2_filled IS NOT NULL AND spO2_filled IS NOT NULL THEN ( spO2_filled / fio2_filled )\n        ELSE NULL\n        END\n        \"\"\"\n    ))\n\ndf = df.distinct()\ndf.write.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\", mode=\"overwrite\")\n\n## Get first time somone on oxygen therapy with PaO2/FiO2 &lt; 200 (S/F &lt; 179 if no P/F measured) and invasive and non-invasive ventilation\n\ndf = df.filter((((f.col(\"p_f\")&lt;200))|\n                (f.col(\"s_f\")&lt;179) |\n                (f.col('device_filled')=='Vent') | \n                (f.col('device_filled')=='NIPPV')))\n\n\ndf = df.select(\"C19_HAR_ID\", \"txnDt\", \"device_filled\",\"pao2_filled\",\"fio2_filled\", \"spO2_filled\", \"p_f\", \"s_f\")\n\nw1 = Window.partitionBy(\"C19_HAR_ID\").orderBy('txnDt')\n\ndf_first_with_time = df.withColumn(\"row\",f.row_number().over(w1)) \\\n             .filter(f.col(\"row\") == 1).drop(\"row\")\n\ndf_first_with_time = df_first_with_time.select(\"C19_HAR_ID\", \"txnDt\").withColumnRenamed(\"txnDt\", \"recorded_time\")\n\nresp_support = df_first_with_time.groupBy(\"C19_HAR_ID\").agg(f.min(\"recorded_time\").alias(\"resp_life_support_start\")).distinct()\n\n\n\n\nUsing the RCLIF_medication_admin_continuous table, we identify the first instance a patient is started on a vasopressor. We inner join the adults hospitalized in the time period to the RCLIF_medication_admin_continuous table, then filter to only vasopressor medications, then take the first time per patient per encounter to use as the index time.\n# Now pressors\ndf_meds = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_meds_admin_conti.csv')\ndf_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_time','yyyy-MM-dd HH:mm:ss'))\n\n# Filter to pressor medications\n\npressors = df_meds.filter(f.col('med_category')=='vasoactives')\npressors = pressors.select(\"C19_HAR_ID\", \"admin_time\")\n\n# Get first time someone is on a pressor\npressors = pressors.groupBy(\"C19_HAR_ID\").agg(f.min(\"admin_time\").alias(\"pressor_life_support_start\"))\npressors = pressors.join(har_ids, on='C19_HAR_ID', how='inner').distinct()\n\n\n\nThe start of life support will be the index time for constructing the 48 hour analytic dataset (41 hours before, the index hour, and 6 hours after). To do that, we take the minimum between the time respiratory criteria was met and the time vasopressors were started. Now we have a cohort identified with the first hour block life support was initiated (index time).\nWe also calculate the start of the window time and end of the window time, and “explode” between these times to create the base 48 hour blocked dataset for the cohort that subsequent data will be left joined to.\n\n# Get first time on life support\n\ndf = pressors.join(resp_support, on='C19_HAR_ID', how='full')\ndf = df.withColumn(\"life_support_start\", f.least(f.col('pressor_life_support_start'),\n                                                 f.col('resp_life_support_start')))\ndf = df.filter(f.col('life_support_start').isNotNull())\n\ndf = df.withColumn('window_start', (f.col('life_support_start')-f.expr(\"INTERVAL 41 HOURS\")))\ndf = df.withColumn('window_end', (f.col('life_support_start')+f.expr(\"INTERVAL 6 HOURS\")))\ndf = df.select(\"C19_HAR_ID\", \"life_support_start\", \"window_start\", \"window_end\")\n\n## Re-join age at admission and disposition, admission & discharge dttm, and zip\ndf = df.join(limited_ids, on='C19_HAR_ID', how='left')\ndf = df.join(demo_disp, on='C19_HAR_ID', how='left')\ndf = df.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \"age_at_admission\",\n               \"disposition\")\n\ndf.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\", mode=\"overwrite\")\n\n## Explode between window to get all hourly timestamps\ncohort_hours = df.withColumn('txnDt', \n                                   f.explode(f.expr('sequence(window_start, window_end, interval 1 hour)')))\ncohort_hours = cohort_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\ncohort_hours = cohort_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\ncohort_hours = cohort_hours.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \n                                   \"age_at_admission\", \"disposition\", \"meas_date\", \"meas_hour\").distinct()\n\ncohort_hours.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\", mode=\"overwrite\")"
  },
  {
    "objectID": "posts/relational_clif/index.html",
    "href": "posts/relational_clif/index.html",
    "title": "Relational CLIF ERD and Data Dictionary",
    "section": "",
    "text": "Relational CLIF has 20 tables that are organized into clinically relevant column categories - demographics, objective measures, respiratory support, orders and inputs-outputs. Below are sample templates for each table in R-CLIF.\n\n\nPatient_encounters\n\n\n\n\n\n\n\n\npatient_id\nencounter_id\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n5\n10\n\n\n6\n11\n\n\n6\n12\n\n\n\n\n\npatient_id is an ID variable for each patient\nencounter_id is an ID variable for each patient encounter (a given patient can have multiple encounters)\n\n\n\nPatient_demographics\n\n\n\n\n\n\n\n\n\n\npatient_id\nrace\nethnicity\nsex\n\n\n\n\n1\nBlack or African-American\nNon-hispanic\nFemale\n\n\n5\nBlack or African-American\nNon-hispanic\nMale\n\n\n6\nWhite\nNon-hispanic\nMale\n\n\n32\nBlack or African-American\nNon-hispanic\nMale\n\n\n43\nWhite\nHispanic\nFemale\n\n\n62\nAsian\nNon-hispanic\nFemale\n\n\n\n\n\npatient_id is an ID variable for each patient\nrace description of patient’s race. Categories include Black or African-American, White, American Indian or Alaska Native, Asian, Native Hawaiian or Other Pacific Islander, Unknown, Other\nethnicity description of patient’s ethnicity. Categories include Hispanic or Non-Hispanic\nsex is the patient’s biological sex - Male or Female\n\n\n\nLimited_identifiers\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nadmission_dttm\ndischarge_dttm\nbirth_dttm\ncoordinates\n\n\n\n\n1\n8/27/2020 8:15:00\n8/27/2020 18:59:00\n8/10/2014\n\n\n\n2\n6/28/2021 7:00:00\n6/27/2021 19:00:00\n2/11/2000\n\n\n\n3\n9/17/2021 8:43:00\n9/17/2021 18:59:00\n2/11/2000\n\n\n\n10\n8/12/2020 00:44:00\n8/12/2020 18:59:00\n4/21/1990\n\n\n\n11\n4/19/2021 6:23:00\n4/19/2021 18:59:00\n1/23/2019\n\n\n\n12\n10/6/2022 10:43:00\n10/6/2022 18:59:00\n1/23/2019\n\n\n\n\n\n\nencounter_id is an ID variable for each patient encounter (a given patient can have multiple encounters)\nadmission_dttm is the date and time the patient is admitted (in the format %Y-%m-%d %H:%M:% )\ndischarge_dttm is the date and time the patient is discharged (in the format %Y-%m-%d %H:%M:% )\nbirth_dttm is the date of birth\ncoordinates provide patient coordinates\n\n\n\nEncounter_demographics_disposition\n\n\n\n\n\n\n\n\n\nencounter_id\nage_at_admission\ndisposition\n\n\n\n\n1\n6\nHome\n\n\n2\n22\nHome\n\n\n3\n2\n\n\n\n10\n20\nDischarged to another facility\n\n\n11\n2\nHome\n\n\n62\n66\nHospice\n\n\n\n\n\nencounter_id is an ID variable for each patient encounter (a given patient can have multiple encounters)\nage_at_admission is the age of the patient at the time of admission (in the format %Y-%m-%d %H:%M:% )\ndisposition is the description of disposition when discharged. Categories include Home, Hospice, Discharged to another facility, Dead, Admitted and Other\n\n\n\nVitals\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nrecorded_time\nvital_name\nvital_value\nmeas_site_name\n\n\n\n\n1\n2022-05-05 04:18:00\nrespiratory_rate\n18\nnot specified\n\n\n1\n2022-05-05 04:18:00\nspO2\n97\nnot specified\n\n\n1\n2022-05-05 04:18:00\nheight\n73\nnot specified\n\n\n1\n2022-05-05 04:18:00\ntemp\n98.1\nnot specified\n\n\n1\n2022-05-05 04:18:00\nheart_rate\n73\nnot specified\n\n\n1\n2022-05-05 04:18:00\nweight\n1756.8\nnot specified\n\n\n\n\n\nencounter_id is an ID variable for each patient encounter ( a given patient can have multiple encounters )\nrecorded_time is the date and time when the vital is recorded\nvital_name includes a limited number of vitals, namely - temp(C), pulse, sbp, dbp, sp02, respiration, map, height_in, weight_kg\nvital_value is the recorded value of the vital identified by the CLIF consortium\nmeas_site_name is the site where vital is recorded. It has three categories - arterial, core, not specified.\n\n\n\nLabs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nlab_order_time\nlab_result_time\nlab_group\nlab_name\nlab_value\nreference_unit\nlab_type_name\n\n\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nCBC\nbasophil\n1\n%\nstandard;poc\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nCBC\nmonocyte\n7\n%\nstandard;poc\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nCBC\nneutrophil\n47\n%\nstandard;poc\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nCBC\nlymphocyte\n44\n%\nstandard;poc\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nCBC\neosinophils\n1\n%\nstandard;poc\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:53:00\nLFT\nbilirubin_unconjugated\n0.9\nmg/dL\nstandard;poc\n\n\n\n\n\nlab_order_time is the date and time when the lab is ordered\nlab_order_time is the date and time when the lab results are available\nlab_group includes a limited number of labs that are categorized into five groups - ABG, BMP, CBC, Coags, LFT, Lactic Acid, Misc, VBG\nlab_name includes a limited number of labs identified by the CLIF consortium\nlab_value is the recorded value corresponding to a lab_name\nreference_unit is the unit of measurement for that lab\nlab_type_name has three categories - arterial, venous and standard/proc\n\n\n\nRespiratory_support\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nrecorded_time\ndevice_name\nmode_name\nmode_category\nlpm\nfiO2\npeep\nset_volume\npressure_support\nset_resp_rate\n\n\n\n\n4\n10/6/2022 10:20:00\nVent\nA/C Volume\nVolume\nNA\n0.4\n4\n400\nNA\n16\n\n\n4\n10/6/2022 11:00:00\nVent\nA/C Volume\nVolume\nNA\n0.4\n4\n400\nNA\n16\n\n\n4\n10/6/2022 15:00:00\nVent\nA/C Volume\nVolume\nNA\n0.4\n4\n400\nNA\n14\n\n\n4\n10/7/2022 6:45:00\nNasal Cannula\nNA\nNA\n4\n0.4\n5\nNA\nNA\n16\n\n\n4\n10/7/2022 16:00:00\nNasal Cannula\nNA\nNA\n2\n0.4\n5\nNA\n0\nNA\n\n\n4\n10/9/2022 5:07:00\nRoom Air\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\nrecorded_time is the date and time when the device started (in the format %Y-%m-%d %H:%M:% )\ndevice_name includes a limited number of devices identified by the CLIF consortium, namely - Vent, NIPPV, High Flow NC, Face Mask, Trach Collar, Nasal Cannula, Room Air, Other\nmode_name includes a limited number of modes identified by the CLIF consortium\nmode_category includes a limited number of mode categories identified by the CLIF consortium, namely - pressure, volume, spontaneous\nlpm is liters per minute\nfiO2 is fraction of inspired O2\npeep is positive-end-expiratory pressure\nset_volume is measured in mL\npressure_support measured in cmH2O\nset_resp_rate measured in bpm\n\n\n\nADT\n\n\n\n\n\n\n\n\n\nencounter_id\nin_time\nlocation_name\n\n\n\n\n1\n8/31/2021 5:02:00 AM\nHome\n\n\n2\n10/6/2022 10:20:00\nHome\n\n\n3\n8/31/2021 5:02:00 AM\nHome\n\n\n10\n7/26/2022 1:57:00 PM\nDischarged to another facility\n\n\n11\n10/6/2022 10:20:00\nHome\n\n\n62\n10/6/2022 10:20:00\nHospice\n\n\n\n\n\nencounter_id is an ID variable for each patient encounter (a given patient can have multiple encounters)\nin_time start date and time at a particular location (in the format %Y-%m-%d %H:%M:% )\nlocation_name is the location of the patient inside the hospital. Categories include ER, OR, ICU, Ward, and Other\n\n\n\nMedication_admin_continuous\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nmed_order_id\nadmin_time\nmed_name\nmed_category\nmed_route\nmed_dose\nmed_dose_unit\n\n\n\n\n2\n43\n10/6/2022 11:10:00\nphenylephrine\nvasoactives\nIntravenous\n0.4\nmcg/kg/min\n\n\n2\n76\n10/6/2022 11:13:00\nphenylephrine\nvasoactives\nIntravenous\n0.75\nmcg/kg/min\n\n\n2\n89\n10/6/2022 11:32:00\ninsulin\nendocrine\nIntravenous\n2\nUnits/hr\n\n\n11\n42\n1/22/2022 00:00:00\npropofol\nsedation\nIntravenous\n40\nmcg/kg/min\n\n\n11\n807\n1/22/2022 02:13:00\npropofol\nsedation\nIntravenous\n30\nmcg/kg/min\n\n\n11\n432\n1/22/2022 04:00:00\nfentanyl\nsedation\nIntravenous\n150\nmcg/hr\n\n\n\n\n\nencounter_id is an ID variable for each patient encounter (a given patient can have multiple encounters)\nmed_order_id a unique ID for med_order placed\nadmin_time start date and time for medication administration\nmed_name medication name\nmed_category medication category\nmed_route eod of medicine delivery\nmed_dose quantity taken in dose\nmed_dose_unit unit of dose"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Meet the CLIF Team",
    "section": "",
    "text": "William Parker, MD, PhD   \n\n\n\n\n\nJay Koyner, MD   \n\n\n\n\n\nKevin Smith, MD   \n\n\n\n\n\nKevin Buell, MBBS   \n\n\n\n\n\nBhakti Patel, MD   \n\n\n\n\n\nKaveri Chhikara, MS   \n\n\n\n\n\nRachel Baccile, MPP   \n\n\n\n\n\nKyle Carey, MS   \n\n\n\n\n\nKyle Carey, MS"
  },
  {
    "objectID": "about.html#university-of-chicago",
    "href": "about.html#university-of-chicago",
    "title": "Meet the CLIF Team",
    "section": "",
    "text": "William Parker, MD, PhD   \n\n\n\n\n\nJay Koyner, MD   \n\n\n\n\n\nKevin Smith, MD   \n\n\n\n\n\nKevin Buell, MBBS   \n\n\n\n\n\nBhakti Patel, MD   \n\n\n\n\n\nKaveri Chhikara, MS   \n\n\n\n\n\nRachel Baccile, MPP   \n\n\n\n\n\nKyle Carey, MS   \n\n\n\n\n\nKyle Carey, MS"
  },
  {
    "objectID": "about.html#rush-university",
    "href": "about.html#rush-university",
    "title": "Meet the CLIF Team",
    "section": "Rush University",
    "text": "Rush University\n\n\n\n\nJ.C. Rojas, MD      \n\n\n\n\n\nVaishvik C., MS"
  },
  {
    "objectID": "about.html#northwestern-university",
    "href": "about.html#northwestern-university",
    "title": "Meet the CLIF Team",
    "section": "Northwestern University",
    "text": "Northwestern University\n\n\n\n\nCatherine Gao, MD   \n\n\n\n\n\nYuan Luo, PhD   \n\n\n\n\n\nChengsheng Mao, PhD   \n\n\n\n\n\nSaki Amagai, PhD"
  },
  {
    "objectID": "about.html#ohsu",
    "href": "about.html#ohsu",
    "title": "Meet the CLIF Team",
    "section": "OHSU",
    "text": "OHSU\n\nPat Lyons, MD\nBrenna Park-Egan, MS"
  },
  {
    "objectID": "about.html#john-hopkins-university",
    "href": "about.html#john-hopkins-university",
    "title": "Meet the CLIF Team",
    "section": "John Hopkins University",
    "text": "John Hopkins University\n\n\n\n\nChad Hochberg, MD"
  },
  {
    "objectID": "about.html#university-of-minnesota",
    "href": "about.html#university-of-minnesota",
    "title": "Meet the CLIF Team",
    "section": "University of Minnesota",
    "text": "University of Minnesota\n\n\n\n\nNicholas Ingraham, MD"
  },
  {
    "objectID": "about.html#tufts-university",
    "href": "about.html#tufts-university",
    "title": "Meet the CLIF Team",
    "section": "Tufts University",
    "text": "Tufts University\n\n\n\n\nSusan Han, MD"
  },
  {
    "objectID": "about.html#emory-university",
    "href": "about.html#emory-university",
    "title": "Meet the CLIF Team",
    "section": "Emory University",
    "text": "Emory University\n\n\n\n\nSiva Bhavani, MD   \n\n\n\n\n\nMuna Nour, MPH"
  },
  {
    "objectID": "posts/sipa_blog/SIPA_blog_post.html#constructing-a-48-hour-analytic-dataset",
    "href": "posts/sipa_blog/SIPA_blog_post.html#constructing-a-48-hour-analytic-dataset",
    "title": "Constructing an Analytic Dataset from CLIF",
    "section": "2. Constructing a 48 Hour Analytic Dataset",
    "text": "2. Constructing a 48 Hour Analytic Dataset\n\n2.1 48 Hour Labs\nTo construct the 48 hour blocked labs dataset, we: 1. Read in RCLIF_labs and cohort identified in previous section 2. Left join RCLIF_labs to cohort 3. Extract the date and hour of each lab, find minimum and maximum per hour per person per encounter\n## Read in Labs\nlabs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\nlabs = labs.withColumn(\"lab_value\",labs.lab_value.cast('double'))\nlabs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n\nlabs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\nlabs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\nlabs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n\n## Left join to cohort\ncohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\ncohort = cohort.withColumn('life_support_start_time',f.to_timestamp('life_support_start','yyyy-MM-dd HH:mm:ss'))\ncohort = cohort.select('C19_HAR_ID', 'life_support_start_time')\n\ncohort_labs = cohort.join(labs, on=\"C19_HAR_ID\", how=\"left\")\n\ncohort_labs = cohort_labs.select('C19_HAR_ID', 'life_support_start_time','meas_date', 'meas_hour', \n                                       'lab_name', 'lab_value')\ncohort_labs = cohort_labs.filter(f.col('lab_name').isNotNull())\ncohort_labs = cohort_labs.filter(f.col('lab_value').isNotNull())\n\n## Get min and max for each time lab\ngroup_cols = [\"C19_HAR_ID\", \"life_support_start_time\",'meas_date', 'meas_hour']\ncohort_labs_wide = cohort_labs.groupBy(group_cols) \\\n                                     .pivot(\"lab_name\") \\\n                                     .agg(f.min('lab_value').alias(\"min\"),\n                                         f.max('lab_value').alias(\"max\")).orderBy(group_cols)\nNext, we:\n\nLeft join the hourly min and max of all the labs from the previous step to the hourly blocked cohort dataset\nForward fill lab values. For this example, I have only forward filled the three lab values we wanted to use in a 48 hour analytic dataset for SIPA (creatinine, bilirubin, and platelet count). Future work will include making this a function, and filling all min and max lab values.\n\n\n## Join to cohort hourly blocked dataset\ncohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\")\n\ngroup_cols = [\"C19_HAR_ID\", 'life_support_start', 'meas_date', 'meas_hour']\ncohort_labs_48 = cohort_hours.join(cohort_labs_wide, on=group_cols, how='left')\n\n## Carry forward labs we need for SIPA\n\ncohort_labs_48 = cohort_labs_48.withColumn('billirubin_max_filled', \n                                       f.coalesce(f.col('bilirubin_total_max'), \n                                                  f.last('bilirubin_total_max', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy(group_cols)), f.lit('NULL')))\n\n\ncohort_labs_48 = cohort_labs_48.withColumn('platelet_count_min_filled', \n                                       f.coalesce(f.col('platelet_count_min'), \n                                                  f.last('platelet_count_min', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy(group_cols)), f.lit('NULL')))\n\n\ncohort_labs_48 = cohort_labs_48.withColumn('creatinine_max_filled', \n                                       f.coalesce(f.col('creatinine_max'), \n                                                  f.last('creatinine_max', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy(group_cols)), f.lit('NULL')))\n\n\n\ncohort_labs_48.write.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48.parquet\", mode=\"overwrite\")\nLastly, we summarize the values of the additional labs we want for SIPA (BUN, pH, potassium) and save a dataset of one observation per person/encounter with the worst values of all selected labs.\n## Summarize to one row per person\ngroup_cols = ['C19_HAR_ID', 'life_support_start', 'window_start', 'window_end']\ncohort_labs_48_summary = cohort_labs_48.groupBy(group_cols) \\\n                                     .agg(f.min('creatinine_max_filled').alias(\"creatinine_max_filled\"),\n                                         f.max('platelet_count_min_filled').alias(\"platelet_count_min_filled\"),\n                                         f.max('billirubin_max_filled').alias(\"billirubin_max_filled\"),\n                                         f.max('potassium_max').alias(\"potassium_max\"),\n                                         f.max('bun_max').alias(\"bun_max\"),\n                                         f.min('ph_venous_min').alias(\"ph_venous_min\"))\\\n                                    .distinct()\\\n                                    .orderBy(\"life_support_start\")\n\ncohort_labs_48_summary.write.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48_summary.parquet\", \n                                       mode=\"overwrite\")\n\n\n2.2 48 Hour Vitals\nWe repeat the process to create the 48 hour dataset for vitals. For vitals, we only carry forward height and weight. We also calculate MAP using the hourly SBP and DBP when MAP is not available before summarizing to one obervation per person/encounter to ensure the SBP and DBP are measured within the same hour.\n## Read in Vitals\nvitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals.parquet\")\nvitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n\nvitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\nvitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\nvitals = vitals.withColumn(\"vital_value\",vitals.vital_value.cast('double'))\n\n## Left join to cohort\ncohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\ncohort = cohort.withColumn('life_support_start',f.to_timestamp('life_support_start','yyyy-MM-dd HH:mm:ss'))\ncohort = cohort.withColumn('window_start',f.to_timestamp('window_start','yyyy-MM-dd HH:mm:ss'))\ncohort = cohort.withColumn('window_end',f.to_timestamp('window_end','yyyy-MM-dd HH:mm:ss'))\n\ncohort_vitals = cohort.join(vitals, on=\"C19_HAR_ID\", how=\"left\")\ncohort_vitals = cohort_vitals.filter(f.col('vital_name').isNotNull())\ncohort_vitals = cohort_vitals.filter(f.col('vital_value').isNotNull())\ncohort_vitals = cohort_vitals.filter(f.col('vital_value')&gt;=0)\n\n\n## Filter to only vitals in window\ncohort_vitals_window = cohort_vitals.filter((f.col('measured_time') &gt;= f.col('window_start')) &\n                              (f.col('measured_time') &lt;= f.col('window_end')))\n\ncohort_vitals_wide = cohort_vitals_window.select('C19_HAR_ID', 'life_support_start','meas_date', 'meas_hour', \n                                       'vital_name', 'vital_value')\n\n## Get min and max for each time vital\ngroup_cols = [\"C19_HAR_ID\", \"life_support_start\",'meas_date', 'meas_hour']\ncohort_vitals_wide = cohort_vitals_wide.groupBy(group_cols) \\\n                                     .pivot(\"vital_name\") \\\n                                     .agg(f.min('vital_value').alias(\"min\"),\n                                         f.max('vital_value').alias(\"max\")).orderBy(group_cols)\n\n\n## Join to cohort hourly blocked dataset\ncohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\")\n\ngroup_cols = [\"C19_HAR_ID\", 'life_support_start', 'meas_date', 'meas_hour']\ncohort_vitals_48 = cohort_hours.join(cohort_vitals_wide, on=group_cols, how='left')\n\n\n## Carry forward height and weight only for SIPA\ncohort_vitals_48 = cohort_vitals_48.withColumn('weight_filled', \n                                       f.coalesce(f.col('weight_max'), \n                                                  f.last('weight_max', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\n\ncohort_vitals_48 = cohort_vitals_48.withColumn('height_filled', \n                                       f.coalesce(f.col('height_max'), \n                                                  f.last('height_max', True) \\\n                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n                                                        .orderBy('txnDt')), f.lit('NULL')))\n\n\n## Fill in MAP using SBP and DBP--these need to be measured in same hour, so need to calculate before summarizing \ncohort_vitals_48 = cohort_vitals_48.withColumn(\"MAP_for_sofa\", f.expr(\n        \"\"\"\n        CASE\n        WHEN MAP_min IS NOT NULL THEN MAP_min\n        WHEN MAP_min IS NULL AND sbp_min IS NOT NULL AND dbp_min IS NOT NULL THEN ( sbp_min + 2.0 * dbp_min ) / 3.0\n        ELSE NULL\n        END\n        \"\"\"\n    ))\ncohort_vitals_48.write.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48.parquet\", mode=\"overwrite\")\n\n\n## Summarize to one observation per person/encounter\ngroup_cols = ['C19_HAR_ID', 'life_support_start', 'window_start', 'window_end']\n\ncohort_vitals_48_summary = cohort_vitals_48.groupBy(group_cols) \\\n                                     .agg(f.max('weight_filled').alias(\"weight_filled\"),\n                                         f.max('height_filled').alias(\"height_filled\"),\n                                         f.min('MAP_for_sofa').alias(\"MAP_for_sofa\"))\\\n                                    .distinct()\\\n                                    .orderBy(\"life_support_start\")\n\ncohort_vitals_48_summary.write.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48_summary.parquet\", \n                                         mode=\"overwrite\")\n\n\n2.3 Combining respiratory support, vitals, and medication to determine hours of life support received\nWe start with our base hourly blocked dataset for our cohort. We then left join the hourly blocked respiratory support and hourly blocked vitals created previously. Next, we bring in vasopressor medications and sum the number of vasopressors received per hour and whether it was dobutamine alone.\nWe capped the number of pressors received at 4, likely because they were being transitioned off some drugs and onto others. Clinically more than 4 would not be used at one time, so we set that as the maximum.\n\n## Read in cohort hourly blocked base dataset\ngroup_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\ncohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\").orderBy(group_cols)\n\n## Bring in respiratory support hourly blocked dataset\nresp_support = spark.read.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\").orderBy(group_cols)\nresp_support = resp_support.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'device_filled', 'fio2_filled', 'pao2_filled', 'spO2_filled', 'p_f', 's_f')\n\n## Left join respiratory support to cohort hourly blocked\ncohort_resp_support = cohort_hours.join(resp_support, on=group_cols, how=\"left\").orderBy(group_cols)\n\n# Bring in worst vitals. Only need MAP for SIPA\nvitals = spark.read.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48.parquet\")\nvitals = vitals.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'MAP_for_SOFA')\n\ncohort_resp_vitals = cohort_resp_support.join(vitals, on=group_cols, how=\"left\").orderBy(group_cols)\n\n## bring in meds to get flag for life support yes/no\n# Now pressors\ndf_meds = spark.read.parquet('/project2/wparker/SIPA_data/RCLIF_medication_admin_continuous.parquet')\ndf_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_dttm','yyyy-MM-dd HH:mm:ss'))\ndf_meds = df_meds.withColumnRenamed(\"encounter_id\", \"C19_HAR_ID\")\n\n# Filter to pressor medications\n\npressors = df_meds.filter(f.col('med_category')=='vasoactives')\npressors = pressors.select(\"C19_HAR_ID\", \"admin_time\", \"med_name\")\n\n## Get cohort index times\ncohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\ncohort = cohort.select('C19_HAR_ID', 'window_start', 'window_end').distinct()\n\n## Get all pressors in 48 hour window\ncohort_pressors = cohort.join(pressors,'C19_HAR_ID','left')\n\ncohort_pressors = cohort_pressors.filter((f.col('admin_time') &gt;= f.col('window_start')) &\n                              (f.col('admin_time') &lt;= f.col('window_end')))\n\ncohort_pressors_48 = cohort_pressors.withColumn('meas_hour', f.hour(f.col('admin_time')))\ncohort_pressors_48 = cohort_pressors_48.withColumn('meas_date', f.to_date(f.col('admin_time')))\n\ncohort_pressors_48 = cohort_pressors_48.select('C19_HAR_ID','meas_hour','meas_date', 'med_name').distinct()\n\n## Get number of pressors per hour\nw2 = Window.partitionBy(group_cols).orderBy(\"med_name\")\n\ngroup_cols = ['C19_HAR_ID','meas_hour','meas_date', 'med_name']\n\ncohort_pressors_grouped = cohort_pressors_48.withColumn(\"row\",f.row_number().over(w2))\ncohort_pressors_grouped = cohort_pressors_48.withColumn(\"count\",f.lit(1))\n\n## Get count of pressors per hour\ngroup_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\ncohort_pressors_grouped = cohort_pressors_grouped.groupBy(group_cols) \\\n                                     .pivot(\"med_name\") \\\n                                     .agg(f.count('count').alias(\"count\")).orderBy(group_cols)\n\ncohort_pressors_grouped = cohort_pressors_grouped.withColumn(\"dobutamine_alone\", f.expr(\n        \"\"\"\n        CASE\n        WHEN dobutamine IS NOT NULL AND dopamine IS NULL AND epinephrine IS NULL AND isoproterenol IS NULL AND milrinone IS NULL AND norepinephrine IS NULL AND phenylephrine IS NULL AND vasopressin IS NULL THEN 1\n        ELSE 0\n        END\n        \"\"\"\n    ))\n\ncol_list = ['dobutamine', 'dopamine', 'epinephrine', 'isoproterenol', 'milrinone', 'norepinephrine', 'phenylephrine', 'vasopressin']\ncohort_pressors_grouped = cohort_pressors_grouped.na.fill(0, subset=col_list)\ncohort_pressors_grouped = cohort_pressors_grouped.withColumn('num_pressors', sum([f.col(c) for c in col_list]))\n\ncohort_pressors_grouped = cohort_pressors_grouped.withColumn(\"num_pressors\", f.expr(\n        \"\"\"\n        CASE\n        WHEN num_pressors &gt; 4 THEN 4\n        ELSE num_pressors\n        END\n        \"\"\"\n    ))\ncohort_pressors_grouped = cohort_pressors_grouped.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'num_pressors', 'dobutamine_alone')\n\ngroup_cols = ['C19_HAR_ID','meas_date','meas_hour']\ncohort_resp_vitals_pressors = cohort_resp_vitals.join(cohort_pressors_grouped, on=group_cols, how=\"left\")\n\ncol_list = ['num_pressors', 'dobutamine_alone']\ncohort_resp_vitals_pressors = cohort_resp_vitals_pressors.na.fill(0, subset=col_list)\nNext, we want to exclude people who received fewer than 6 hours of life support. So we create a flag for if someone received life support in that hour, defined as receiving vasopressor medications, non-invasive or invasive mechanical ventilation, a PaO2/FiO2 ratio less than 200, or an SpO2/FiO2 ratio less than 179. We sum and filter out those receiveing less than 6 hours of life support.\n\n##Flag if on life support at each hour\n\ncohort_resp_vitals_pressors = cohort_resp_vitals_pressors.withColumn(\"on_life_support\", f.expr(\n        \"\"\"\n        CASE\n        WHEN num_pressors &gt;=1 THEN 1\n        WHEN device_filled == 'NIPPV' THEN 1\n        WHEN device_filled == 'Vent' THEN 1\n        WHEN  p_f &lt; 200 THEN 1\n        WHEN s_f &lt; 179 THEN 1\n        ELSE 0\n        END\n        \"\"\"\n    ))\n\ncohort_life_support_count = cohort_resp_vitals_pressors.groupBy('C19_HAR_ID') \\\n                                            .agg(f.sum('on_life_support').alias('life_support_count')) \\\n                                            .filter(f.col('life_support_count')&gt;=6)\n\ndf = cohort_life_support_count.join(cohort_resp_vitals_pressors, on='C19_HAR_ID', how=\"left\")\n\n\n2.4 Merging scores, labs, and dialysis flag\nLastly, we repeat a similar procedure to left join hourly blocked minimum scores, labs, and a flag for whether a person was receiving dialysis.\n\n## add glasgow score\nscores = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_scores_10192023.csv')\nscores = scores.withColumn('score_time',f.to_timestamp('score_time','yyyy-MM-dd HH:mm:ss'))\nscores = scores.withColumn('meas_hour', f.hour(f.col('score_time')))\nscores = scores.withColumn('meas_date', f.to_date(f.col('score_time')))\nscores = scores.withColumn(\"score_value\",scores.score_value.cast('double'))\n\n## Get min GCS\ngroup_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\nscores_wide = scores.groupBy(group_cols) \\\n                                     .pivot(\"score_name\") \\\n                                     .agg(f.min('score_value').alias(\"min\")).orderBy(group_cols)\nscores_wide = scores_wide.withColumnRenamed('NUR RA GLASGOW ADULT SCORING', 'min_gcs_score')\n\ncohort_scores_48 = cohort_hours.join(scores_wide, on=group_cols, how=\"left\")\ncohort_scores_48 = cohort_scores_48.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'min_gcs_score')\n\ndf = df.join(cohort_scores_48, on=group_cols, how=\"left\")\n\n## Add labs\nlabs = spark.read.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48.parquet\")\nlabs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'billirubin_max_filled', 'potassium_max', 'bun_max', 'ph_venous_min',\n                         'platelet_count_min_filled', 'creatinine_max_filled')\ndf = df.join(labs, on=group_cols, how=\"left\")\n\n## add dialysis flag\ndialysis = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_dialysis_flag_only.csv')\ndialysis = dialysis.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\ndialysis = dialysis.withColumn('meas_hour', f.hour(f.col('recorded_time')))\ndialysis = dialysis.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n\ndialysis = dialysis.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'on_dialysis') \\\n                .distinct() \\\n                .groupBy(group_cols) \\\n                .agg(f.max('on_dialysis').alias('on_dialysis'))\ndf = df.join(dialysis, on=group_cols, how=\"left\")\n\ndf.repartition(1).write.csv(\"/project2/wparker/SIPA_data/sipa_df_48.csv\", mode='overwrite', header=\"true\")"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "CLIF Consortium",
    "section": "",
    "text": "The CLIF consortium is a network of NIH funded investigators.\nPrincipal Investigators (PIs) in the consortium are supported by a wide range of NIH grants that use big data and advanced ML techniques to answer open scientific questions related to critical illness. Each of these projects would be better with CLIF rather than done in silos.\n\n\n\nCLIF Project Description: Creating a life support allocation triage score for crisis standards of care\nGrant: R01 LM014263\nPI: William Parker, University of Chicago\n\n\n\n\n\nCLIF Project Description: Studying hospital variation in low-tidal volume ventilation across hospital systems\nGrant: K23 HL166783\nPI: Nicholas Ingraham, University of Minnesota\n\n\n\n\n\nCLIF Project Description: Validating the association of temperature trajectories and ICU mortality across hospital systems\nGrant: K23 GM144867\nPI: Sivasubramanium Bhavani, Emory University\n\n\n\n\n\nCLIF Project Description: Studying the association between proning practice patterns and ARDS survival\nGrant: K23 HL169743\nPI: Chad Hochberg, John Hopkins University\n\n\n\n\n\nCLIF Project Description: Identifying critically ill patients who are eligible for mobilization using the electronic healthcare record\nGrant: K23 HL148387\nPI: Bhakti Patel, University of Chicago\n\n\n\n\n\nCLIF Project Description: Prediction of severe pneumonia using machine learning\nGrant: K23HL169815\nPI: Catherine Gao, Northwestern University\n\n\n\n\n\nCLIF Project Description: Develop an oncology-specific sepsis prediction model using EHR data and human-centered design\nGrant: K08CA270383\nPI: Patrick Lyons, Oregon Health and Science University"
  },
  {
    "objectID": "projects.html#ongoing-projects",
    "href": "projects.html#ongoing-projects",
    "title": "CLIF Consortium",
    "section": "",
    "text": "The CLIF consortium is a network of NIH funded investigators.\nPrincipal Investigators in the consortium are supported by a wide range of NIH grants that use big data and advanced ML techniques to answer open scientific questions related to critical illness. Each of these projects would be better with CLIF rather than done in silos.\n\n\n\nDescription: Creating a life support allocation triage score for crisis standards of care\nGrant: R01 LM014263\nPI: William Parker\n\n\n\n\n\nDescription: Aligning hourly mechanical ventilation data from several U.S. hospitals to determine between-hospital variation in low tidal volume ventilation (LTVV), a crucial evidence-based practice for mechanically ventilated patients.\nFunding:\n\n\n\n\n\nDescription:\nFunding:\n\n\n\n\n\nDescription: Identify and define the duration of time that patients on ventilators are eligible/safe to perform physical/occupational therapy.\nFunding:"
  },
  {
    "objectID": "posts/resp_support/RCLIF_respiratory_support.html#user-input",
    "href": "posts/resp_support/RCLIF_respiratory_support.html#user-input",
    "title": "RCLIF Respiratory Support",
    "section": "USER INPUT",
    "text": "USER INPUT\nProvide file paths\n\nflowsheet_path &lt;- \"data/parquet/C19_FLOW_LDS_part1.parquet\"\nflowsheet2_path &lt;- \"data/parquet/C19_FLOW_LDS_part2.parquet\"\n#CLIF limited vocab\nlimited_vocab_rs_filepath &lt;- \"data/Mode_category_limited_vocab.xlsx\"\n#list of all inpatient encounter. used to filter all other tables in RCLIF.\nall_hb_encounters_filepath &lt;- \"output/all_hb_encounters.csv\"\n\nInput required flowsheet measure names from your data. These are identified with preliminary exploratory data analysis of the flowsheet data at each site, and mapping them to the required RCLIF schema for the respiratory support table.\n\nrequired_flowsheet_names &lt;- c(\n  device_description = \"RT RS OXYGEN DEVICE\",\n  lpm = \"RT RS OXYGEN FLOW\",\n  peep = \"RT RS VENT PRESSURES PEEP/CPAP\",\n  set_volume = \"RT RS VENT VOLUMES VT SET\",\n  exhaled_volume = \"RT RS VENT VOLUMES VE (EXPIRED MINUTE VOLUME)\", \n  pressure_support = \"RT RS VENT PRESSURE PRESSURE SUPPORT\",\n  mode_name = \"RT RS CONVENTIONAL VENT MODES\",\n  set_resp_rate = \"RT RS RESP RATE SET\",\n  ## U of C has multiple entries for fiO2, which are combined later in the code.\n  fiO2_vent = \"RT RS VENT FIO2\",\n  fiO2_rt = \"RT RS FIO2\",\n  fiO2_ni = \"RT RS NI FIO2\"\n  )\n\n\nLoad data\n\n## Update according to the number of files and file format\nflowsheet1 &lt;- read_parquet(here(flowsheet_path))\nflowsheet2 &lt;- read_parquet(here(flowsheet2_path))\nall_flowsheet &lt;- rbind(flowsheet1, flowsheet2)\nrm(flowsheet1)\nrm(flowsheet2)\n\nThe flowsheet dataset used in this script has the following schema\nWe want to convert this long table into the required wide format for the respiratory support table in RCLIF.\nAt this point, each site should explore the all_flowsheet data to identify the required_flowsheet_names and update the user input section above.\n\n\nInpatient encounters list\n\nall_hb_encounters &lt;- read_csv(here(all_hb_encounters_filepath), show_col_types = FALSE)\n\n\n\nRCLIF Limited vocab\n\nrclif_limited_vocab_rs &lt;- read_excel(here(limited_vocab_rs_filepath))"
  },
  {
    "objectID": "posts/resp_support/RCLIF_respiratory_support.html#respiratory-support",
    "href": "posts/resp_support/RCLIF_respiratory_support.html#respiratory-support",
    "title": "RCLIF Respiratory Support",
    "section": "Respiratory support",
    "text": "Respiratory support\nStep 1. Select the required columns, filter to inpatient encounters and required flowsheet names and pivot.\nStep 2. Join with CLIF Limited vocabulary\nStep 3. Assign correct data type to each variable, calculate the final fiO2 value, and create device_name variable according to CLIF limited vocabulary. Some strings in device_description from the raw data may have multiple devices. For this, we follow an order of prioritization: Vent, NIPPV, CPAP, High Flow NC, Nasal Cannula, Trach Collar, Face Mask, Room Air, Other.\n\nwide_rs &lt;- all_flowsheet %&gt;% \n  ## Step 1\n  select(C19_PATIENT_ID, C19_HAR_ID, recorded_time, flo_meas_name, meas_value) %&gt;%\n  filter(C19_HAR_ID %in% all_hb_encounters$C19_HAR_ID & flo_meas_name %in% c(required_flowsheet_names)) %&gt;%\n  pivot_wider(names_from = flo_meas_name, values_from = meas_value) %&gt;% \n  rename(!!!required_flowsheet_names) %&gt;% \n  ## Step 2\n  full_join(rclif_limited_vocab_rs, by = \"mode_name\") %&gt;% \n  ##Step 3\n  mutate(recorded_dttm = format(as.POSIXct(recorded_time, origin = \"1970-01-01\"),  \"%Y-%m-%d %H:%M:%S\"),\n         set_volume = as.numeric(set_volume),\n         exhaled_volume = as.numeric(exhaled_volume),\n         pressure_support = as.numeric(pressure_support),\n         set_resp_rate = as.numeric(set_resp_rate),\n         peep = as.numeric(peep),\n         lpm = as.numeric(lpm),\n         fiO2_rt = as.numeric(fiO2_rt),\n         fiO2_vent = as.numeric(fiO2_vent),\n         fiO2_ni = as.numeric(fiO2_ni),\n         # fiO2 calculation- take the first non NA value among the fiO2 columns\n         fi_O2_combine = coalesce(fiO2_rt, fiO2_vent, fiO2_ni),\n         fiO2 = as.numeric(fi_O2_combine)/100,\n         device_name = case_when(\n           is.na(device_description) ~ NA_character_,\n           (grepl('Vent', device_description, ignore.case = TRUE) | grepl('Bag', device_description, ignore.case = TRUE) |\n              grepl('Valve', device_description, ignore.case = TRUE)) &\n             !grepl('Venturi Mask', device_description, ignore.case = TRUE) ~ 'Vent',\n           grepl('Bipap', device_description, ignore.case = TRUE)  ~ 'NIPPV',\n           grepl('CPAP', device_description, ignore.case = TRUE) ~ 'CPAP',\n           grepl('High Flow NC', device_description, ignore.case = TRUE) ~ 'High Flow NC',\n           grepl('Cannula', device_description, ignore.case = TRUE) ~ 'Nasal Cannula',\n           grepl('Trach Collar', device_description, ignore.case = TRUE) ~ 'Trach Collar',\n           grepl('Mask', device_description, ignore.case = TRUE) ~ 'Face Mask',\n           grepl('Room Air', device_description, ignore.case = TRUE) ~ 'Room Air',\n           TRUE ~ 'Other'),\n         # resolve cases when the first string in o2_device is Vent, remaining string includes Venturi Mask\n         device_name = ifelse(map_chr(str_split(tolower(device_description), \n                                                \";\"), ~.[1]) == 'vent', 'Vent', device_name),\n         # fix errors in lpm\n         lpm = case_when(\n          lpm &lt; 0 ~ NA_real_,\n          device_name == 'Room Air' ~ NA_real_,\n          device_name == 'Vent' ~ NA_real_,\n          device_name == 'NIPPV' ~ NA_real_,\n          device_name == 'Trach Collar' ~ NA_real_,\n          device_name == 'Nasal Cannula' & lpm &gt; 6 ~ 6,\n          device_name == 'High Flow' & lpm &gt; 60 ~ 60,\n          lpm &gt; 60 ~ NA_real_,\n          TRUE ~ lpm)) %&gt;% \n   rename(patient_id = C19_PATIENT_ID,\n         encounter_id = C19_HAR_ID) %&gt;% \n  select(patient_id, encounter_id, recorded_dttm, device_description, device_name,\n         mode_name, mode_category, lpm, fiO2, peep, set_volume,exhaled_volume,\n         pressure_support, set_resp_rate) %&gt;% \n  arrange(patient_id, encounter_id, recorded_dttm)"
  },
  {
    "objectID": "posts/github_for_projects/index.html",
    "href": "posts/github_for_projects/index.html",
    "title": "Using GitHub for CLIF projects",
    "section": "",
    "text": "This blog provides a structured approach to setting up GitHub, tailored specifically for our collaborative projects. By standardizing our setup, we can enhance our workflows, ensure consistency across our datasets, and simplify the complexities involved in managing our extensive research data. We’ll start our GitHub journey with Nick’s project on variation in ventilation and use it as an example in this guide.\n\nStep 1: Prepare your environment\nFirst, confirm that GitHub is installed and properly configured on your workstation. Follow the steps on the official GitHub docs\n\n\nStep 2: Fork the repository\nNavigate to the CLIF-1.0 GitHub repository and fork the repository. To fork a repository just means to make a copy that you are going to manage. When you fork a repository you can make changes without affecting the original repository.\nAction: Click the fork button to create your personal copy of the repository.\n\n\n\nStep 3: Clone your forked repository\nYou’ll then see the forked repository on your profile.You can tell that you are on your own forked version because it will have your username and the name you chose for the repository as well as “forked from” in the top left corner. Clone your forked repository. We need to gain remote access to communicate securely with another computer. This will provide us an encrypted connection between the two network end points.\nAction: Click on the green code button to clone the repository.\n\nNote: The process to clone your repository will depend on if you are using GitHub Desktop, Git in R or Git Bash on command line.\n\nIf you are using the command line, follow the steps delineated here- Step 2: Cloning a Forked Repository\nIf you are using GitHub Desktop, follow the setup instructions here- GitHub Desktop\nYou can also link git directly with RStudio. Follow the steps here- Connect RStudio to Git and GitHub\n\n\n\nStep 4: Access your files\nWith your repository cloned, navigate to the “vent_variation” directory. This directory contains the files needed for your project, ensuring you have immediate access to relevant scripts. To pull updated files, follow the steps here to sync the fork branch.\n\n\n\nProject Specific Setup: R Project Initialization\nThe variation in ventilation project script is a .qmd file. To setup the project workflow, follow these steps:\n\nStart by launching RStudio on your system. To create a new project in the RStudio IDE, use the Create Project command (available on the Projects menu and on the global toolbar):\n\n\n\nCreate project in the existing directory and provide the path to your CLIF-1.0 project specific directory. Example- ~/Desktop/CLIF-1.0/projects/vent_variation\n\n\n\n\nYou are now ready to run the vent_variation.qmd file!"
  },
  {
    "objectID": "data-dictionary.html",
    "href": "data-dictionary.html",
    "title": "CLIF Data Dictionary",
    "section": "",
    "text": "Below is the entity-relationship diagram (ERD) that provides an overview of the relational CLIF database structure.\nRelational CLIF has 20 tables that are organized into clinically relevant column categories - demographics, objective measures, respiratory support, orders, and inputs-outputs. Below are sample templates for each table in R-CLIF. Here you can find detailed descriptions of each table and their fields.\nYou can use our custom GPT- CLIF Assistant to learn more about CLIF and develop analysis scripts using Clifford (Synthetic CLIF)"
  },
  {
    "objectID": "data-dictionary.html#patient_encounters",
    "href": "data-dictionary.html#patient_encounters",
    "title": "CLIF Data Dictionary",
    "section": "Patient_encounters",
    "text": "Patient_encounters\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\n\n\n\n\npatient_id\nINT\nID variable for each patient. Every patient assigned a unique identifier is presumed to be a distinct individual\n\n\nencounter_id\nINT\nID variable for each patient encounter (a given patient can have multiple encounters). Each encounter_id represents a unique hospitalization for a patient, capturing the entire duration of the hospital stay. This is the primary key for most other tables\n\n\n\nExample:\n\n\n\n\npatient_id\nencounter_id\n\n\n\n\n1\n1\n\n\n1\n2\n\n\n1\n3\n\n\n5\n10\n\n\n6\n11\n\n\n6\n12"
  },
  {
    "objectID": "data-dictionary.html#patient_demographics",
    "href": "data-dictionary.html#patient_demographics",
    "title": "CLIF Data Dictionary",
    "section": "Patient_demographics",
    "text": "Patient_demographics\n\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nrace\nVARCHAR\nDescription of patient’s race. Each site could have different strings in source data\nBlack , White, American Indian or Alaska Native, Asian, Native Hawaiian or Other Pacific Islander, Unknown, Other\n\n\nethnicity\nVARCHAR\nDescription of patient’s ethnicity\nHispanic, Non-Hispanic, Unknown\n\n\nsex\nVARCHAR\nPatient’s biological sex\nMale , Female, Unknown\n\n\n\nExample:\n\n\n\n\npatient_id\nrace\nethnicity\nsex\n\n\n\n\n1\nBlack\nNon-hispanic\nFemale\n\n\n5\nBlack\nNon-hispanic\nMale\n\n\n6\nWhite\nNon-hispanic\nMale\n\n\n32\nAsian\nNon-hispanic\nMale\n\n\n43\nWhite\nHispanic\nFemale\n\n\n62\nOther\nNon-hispanic\nFemale"
  },
  {
    "objectID": "data-dictionary.html#limited_identifiers",
    "href": "data-dictionary.html#limited_identifiers",
    "title": "CLIF Data Dictionary",
    "section": "Limited_identifiers",
    "text": "Limited_identifiers\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter. Each encounter_id represents a unique hospitalization for a patient, capturing the entire duration of the hospital stay\n\n\nadmission_dttm\nDATETIME\nDate and time the patient is admitted (in the format %Y-%m-%d %H:%M:%S). Use this date to determine the start date and time of the patient hospitalization\n\n\ndischarge_dttm\nDATETIME\nDate and time the patient is discharged (in the format %Y-%m-%d %H:%M:%S). Use this date to determine the distacharge date of the patient hospitalization\n\n\nbirth_date\nDATETIME\nPatient date of birth. This variable is used to calculate age at admission for analysis\n\n\nzipcode_9digit\nVARCHAR\nPatient zipcode. This variable is used to link the database with other indices like ADI, SVI etc\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nadmission_dttm\ndischarge_dttm\nbirth_dttm\nzipcode_9digit\n\n\n\n\n1\n2020-08-27 08:15:00\n2020-08-27 18:59:00\n2014-08-10\n\n\n\n2\n2021-06-28 07:00:00\n2021-06-27 19:00:00\n2000-02-11\n\n\n\n3\n2021-09-17 08:43:00\n2021-09-17 18:59:00\n2000-02-11\n\n\n\n10\n2020-08-12 00:44:00\n2020-08-12 18:59:00\n1990-04-21\n\n\n\n11\n2021-04-19 06:23:00\n2021-04-19 18:59:00\n2019-01-23\n\n\n\n12\n2022-10-06 10:43:00\n2022-10-06 18:59:00\n2019-01-23"
  },
  {
    "objectID": "data-dictionary.html#encounter_demographics_disposition",
    "href": "data-dictionary.html#encounter_demographics_disposition",
    "title": "CLIF Data Dictionary",
    "section": "Encounter_demographics_disposition",
    "text": "Encounter_demographics_disposition\n\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter.\n\n\n\nage_at_admission\nINT\nAge of the patient at the time of admission. Calculated using the admission_dttm and birth_date from the limited identifiers table.\n\n\n\ndisposition_name\nVARCHAR\nOriginal disposition name string recorded in the raw data. This field allows for the storing of the dispostion value as it appears in the source data. This field is not used for analysis.\n\n\n\ndisposition_category\nVARCHAR\nDescription of disposition when discharged. Map source values stored in disposition_name to the mCIDE categories.\nHome, Hospice, Discharged to another facility, Dead, Admitted, Other\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\nencounter_id\nage_at_admission\ndisposition_name\ndisposition_category\n\n\n\n\n1\n6\nDischarged to Home or Self Care (Routine Discharge)\nHome\n\n\n2\n22\nDischarged/transferred to Home Under Care of Organized Home Health Service Org\nHome\n\n\n3\n54\nLeft Against Medical Advice or Discontinued Care\nHome\n\n\n10\n20\nDischarged/transferred to a Short-Term General Hospital for Inpatient Care\nDischarged to another facility\n\n\n11\n2\nDischarged/transferred to a Facility that Provides Custodial or Supportive Care\nDischarged to another facility\n\n\n62\n66\nHospice - Medical Facility (Certified) Providing Hospice Level of Care\nHospice\n\n\n634\n827\nExpired\nDead"
  },
  {
    "objectID": "data-dictionary.html#dialysis",
    "href": "data-dictionary.html#dialysis",
    "title": "CLIF Data Dictionary",
    "section": "Dialysis",
    "text": "Dialysis\n\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nstart_dttm\nDATETIME\nStart date and time of dialysis\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nstop_dttm\nDATETIME\nStop date and time of dialysis\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\ndialysis_type\nVARCHAR\nType of dialysis performed\n\n\n\ndialysate_flow_amount\nDOUBLE\nAmount of dialysate flow\n\n\n\nultrafiltration_amount\nDOUBLE\nAmount of ultrafiltration\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nstart_dttm\nstop_dttm\ndialysis_type\ndialysate_flow_amount\nultrafiltration_amount\n\n\n\n\n18\n2021-03-24 09:00:00\n2021-03-24 10:00:00\ncrrt\n0.86\n0\n\n\n18\n2021-03-24 10:00:00\n2021-03-24 11:00:00\ncrrt\n3.89\n0\n\n\n18\n2021-03-24 11:00:00\n2021-03-24 12:00:00\ncrrt\n3.82\n0\n\n\n18\n2021-03-24 12:00:00\n2021-03-24 13:00:00\ncrrt\n3.89\n0\n\n\n18\n2021-03-24 13:00:00\n2021-03-24 14:00:00\ncrrt\n3.89\n0\n\n\n18\n2021-03-24 14:00:00\n2021-03-24 15:00:00\ncrrt\n3.900\n0"
  },
  {
    "objectID": "data-dictionary.html#adt",
    "href": "data-dictionary.html#adt",
    "title": "CLIF Data Dictionary",
    "section": "ADT",
    "text": "ADT\n\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nhospital_id\nVARCHAR\nAssign an ID to each hospital in the hospital system\n\n\n\nin_dttm\nDATETIME\nStart date and time at a particular location\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nout_dttm\nDATETIME\nEnd date and time at a particular location\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nlocation_name\nVARCHAR\nLocation of the patient inside the hospital. This field is used to store the patient location from the source data. This field is not used for analysis.\nNo restriction\n\n\nlocation_category\nVARCHAR\nMap location_name from the source data to categories identified under CLIF.\nER, OR, ICU, Ward, Other\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nhospital_id\nin_dttm\nout_dttm\nlocation_name\nlocation_category\n\n\n\n\n1\nA\n2020-12-28 10:35:00\n2020-12-29 03:21:00\nED CCD\nER\n\n\n1\nA\n2020-12-29 03:21:00\n2020-12-29 07:25:00\nN03W\nWard\n\n\n3\nA\n2021-03-18 05:02:00\n2021-03-19 20:22:00\nN03W\nICU\n\n\n3\nB\n2021-03-19 20:22:00\n2021-03-22 09:30:00\nT5SW\nWard\n\n\n11\nF\n2022-09-30 17:50:00\n2022-09-30 23:30:00\nER COMER\nER"
  },
  {
    "objectID": "data-dictionary.html#vitals",
    "href": "data-dictionary.html#vitals",
    "title": "CLIF Data Dictionary",
    "section": "Vitals",
    "text": "Vitals\n\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter.\n\n\n\nrecorded_dttm\nDATETIME\nDate and time when the vital is recorded.\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nvital_name\nVARCHAR\nThis field is used to store the description of the flowsheet measure from the source data. This field is not used for analysis.\nNo restriction\n\n\nvital_category\nVARCHAR\nMap flowsheet measures stored in vital_name to the to categories identified under CLIF.\ntemp_c, pulse, sbp, dbp, spo2, respiratory_rate, map, height_inches, weight_kg\n\n\nvital_value\nDOUBLE\nRecorded value of the vital. Ensure that the measurement unit is aligned with the permissible units of measurements.\ntemp_c = Celsius, height_inches = Inch, weight_kg = Kg, map = mm/Hg, spo2 = %, No unit for pulse, sbp, dbp, and respiratory_rate\n\n\nmeas_site_name\nVARCHAR\nSite where vital is recorded. It has three categories - arterial, core, not specified\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nrecorded_dttm\nvital_name\nvital_category\nvital_value\nmeas_site_name\n\n\n\n\n1\n2022-05-05 04:18:00\nRESPIRATIONS\nrespiratory_rate\n18\nnot specified\n\n\n1\n2022-05-05 04:18:00\nPULSE OXIMETRY\nspo2\n97\nnot specified\n\n\n1\n2022-05-05 04:18:00\nNUR RS CORE TEMPERATURE MEASUREMENT\ntemp_c\n98.1\ncore\n\n\n1\n2022-05-05 04:18:00\nPULSE\npulse\n73\nnot specified\n\n\n1\n2022-05-01 11:23:00\nWEIGHT/SCALE\nweight_kg\n78.8\nnot specified\n\n\n1\n2022-05-01 11:23:00\nHEIGHT\nheight_inches\n73\nnot specified"
  },
  {
    "objectID": "data-dictionary.html#labs",
    "href": "data-dictionary.html#labs",
    "title": "CLIF Data Dictionary",
    "section": "Labs",
    "text": "Labs\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nlab_order_dttm\nDATETIME\nDate and time when the lab is ordered\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nlab_collect_dttm\nDATETIME\nDate and time when the specimen is collected\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nlab_result_dttm\nDATETIME\nDate and time when the lab results are available\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nlab_name\nVARCHAR\nOriginal lab name string recorded in the raw data. This field is not used for analysis.\n\n\n\nlab_category\nVARCHAR\n43 labs identified by the CLIF consortium\nList of lab categories in CLIF\n\n\nlab_group\nVARCHAR\nLab categories roll up to form lab groups\nABG, BMP, CBC, Coags, LFT, Lactic Acid, Misc, VBG\n\n\nlab_value\nDOUBLE\nRecorded value corresponding to a lab\n\n\n\nreference_unit\nVARCHAR\nUnit of measurement for that lab\nPermissible reference values for each lab_category listed here\n\n\nlab_type_name\nVARCHAR\nFollowing categories -\narterial, venous, standard, poc\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nlab_order_dttm\nlab_collect_dttm\nlab_result_dttm\nlab_name\nlab_group\nlab_category\nlab_value\nreference_unit\nlab_type_name\n\n\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nBASOPHILS\nCBC\nbasophil\n1\n%\nstandard\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nMONOCYTES\nCBC\nmonocyte\n7\n%\nstandard\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nNEUTROPHILS\nCBC\nneutrophil\n47\n%\nstandard\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nLYMPHOCYTES\nCBC\nlymphocyte\n44\n%\nstandard\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nEOSINOPHILS\nCBC\neosinophils\n1\n%\nstandard\n\n\n2\n2022-09-30 17:50:00\n2022-09-30 18:05:00\n2022-09-30 18:53:00\nBILIRUBIN, UNCONJUGATED\nLFT\nbilirubin_unconjugated\n0.9\nmg/dL\nstandard\n\n\n\n\nNote: The lab_value field often has non-numeric entries that are useful to make project-specific decisions. A site may choose to keep the lab_value field as a character and create a new field lab_value_numeric that only parses the character field to extract the numeric part of the string."
  },
  {
    "objectID": "data-dictionary.html#respiratory_support",
    "href": "data-dictionary.html#respiratory_support",
    "title": "CLIF Data Dictionary",
    "section": "Respiratory_support",
    "text": "Respiratory_support\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nrecorded_dttm\nDATETIME\nDate and time when the device started\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\ndevice_name\nVARCHAR\nIncludes raw string of the devices. Not used for analysis\nExample mapping for device name to device category\n\n\ndevice_category\nVARCHAR\nIncludes a limited number of devices identified by the CLIF consortium\nVent, NIPPV, CPAP, High Flow NC, Face Mask, Trach Collar, Nasal Cannula, Room Air, Other\n\n\nmode_name\nVARCHAR\nIncludes raw string of the modes. Not used for analysis\nExample mapping for mode name to mode category\n\n\nmode_category\nVARCHAR\nLimited number of modes identified by the CLIF consortium\nAssist Control-Volume Control, Pressure Support/CPAP, Pressure Control, Pressure-Regulated Volume Control, Other, SIMV, Blow by\n\n\ntracheostomy\nBOOLEAN\nIndicates if tracheostomy is performed\n0 = No, 1 = Yes\n\n\nfio2_set\nDOUBLE\nFraction of inspired oxygen set\n\n\n\nlpm_set\nDOUBLE\nLiters per minute set\n\n\n\ntidal_volume_set\nDOUBLE\nTidal volume set (in mL)\n\n\n\nresp_rate_set\nDOUBLE\nRespiratory rate set (in bpm)\n\n\n\npressure_control_set\nDOUBLE\nPressure control set (in cmH2O)\n\n\n\npressure_support_set\nDOUBLE\nPressure support set (in cmH2O)\n\n\n\nflow_rate_set\nDOUBLE\nFlow rate set\n\n\n\npeak_inspiratory_pressure_set\nDOUBLE\nPeak inspiratory pressure set (in cmH2O)\n\n\n\ninspiratory_time_set\nDOUBLE\nInspiratory time set (in seconds)\n\n\n\npeep_set\nDOUBLE\nPositive-end-expiratory pressure set (in cmH2O)\n\n\n\ntidal_volume_obs\nDOUBLE\nObserved tidal volume (in mL)\n\n\n\nresp_rate_obs\nDOUBLE\nObserved respiratory rate (in bpm)\n\n\n\nplateau_pressure_obs\nDOUBLE\nObserved plateau pressure (in cmH2O)\n\n\n\npeak_inspiratory_pressure_obs\nDOUBLE\nObserved peak inspiratory pressure (in cmH2O)\n\n\n\npeep_obs\nDOUBLE\nObserved positive-end-expiratory pressure (in cmH2O)\n\n\n\nminute_vent_obs\nDOUBLE\nObserved minute ventilation (in liters)\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nrecorded_dttm\ndevice_name\ndevice_category\nmode_name\nmode_category\ntracheostomy\nfio2_set\nlpm_set\ntidal_volume_set\nresp_rate_set\npressure_control_set\npressure_support_set\nflow_rate_set\npeak_inspiratory_pressure_set\ninspiratory_time_set\npeep_set\ntidal_volume_obs\nresp_rate_obs\nplateau_pressure_obs\npeak_inspiratory_pressure_obs\npeep_obs\nminute_vent_obs\n\n\n\n\n5\n2024-05-05 19:37:26\n\nNasal Cannula\n\n\n0\n\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5\n2024-05-05 20:37:26\n\nHigh Flow NC\n\n\n0\n100\n60\n\n\n\n\n60\n\n\n\n\n\n\n\n\n\n\n\n5\n2024-05-05 21:37:26\n\nHigh Flow NC\n\n\n0\n100\n60\n\n\n\n\n60\n\n\n\n\n\n\n\n\n\n\n\n5\n2024-05-05 22:37:26\n\nVent\n\nAssist-Control/Volume Control\n0\n100\n\n500\n20\n\n0\n\n1.2\n5\n400\n14\n30\n35\n5\n7\n\n\n\n5\n2024-05-05 23:37:26\n\nVent\n\nAssist-Control/Volume Control\n0\n80\n\n400\n14\n\n0\n\n1.2\n5\n400\n14\n\n\n\n7\n\n\n\n5\n2024-05-06 00:37:26\n\nVent\n\nAssist-Control/Volume Control\n0\n75\n\n400\n14\n\n0\n\n1.2\n5\n400\n22\n\n\n\n\n\n\n\n5\n2024-05-06 03:37:26\n\nVent\n\nAssist-Control/Volume Control\n0\n50\n\n400\n14\n\n0\n\n1.2\n5\n400\n2\n30\n35\n\n7\n\n\n\n5\n2024-05-06 04:37:26\n\nVent\n\nPressure Support\n0\n50\n\n\n\n0\n\n\n\n\n5\n\n\n\n\n\n\n\n\n5\n2024-05-06 05:37:26\n\nNasal Cannula\n\n\n0\n\n4\n\n\n\n\n\n\n\n\n\n20"
  },
  {
    "objectID": "data-dictionary.html#medication_admin_continuous",
    "href": "data-dictionary.html#medication_admin_continuous",
    "title": "CLIF Data Dictionary",
    "section": "Medication_admin_continuous",
    "text": "Medication_admin_continuous\n\n\n\nVariable Name\nData Type\nDefinition\nPermissible Values\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\n\nmed_order_id\nVARCHAR\nMedication order id. Foreign key to link this table to other medication tables\n\n\n\nadmin_dttm\nDATETIME\nDate and time when the medicine was administered\nDatetime format should be %Y-%m-%d %H:%M:%S\n\n\nmed_name\nVARCHAR\nOriginal med name string recorded in the raw data for a limited number of labs identified by the CLIF consortium\nExample mapping of med_name to med_category\n\n\nmed_category\nVARCHAR\nLimited number of medication categories identified by the CLIF consortium\nList of continuous medication categories in CLIF\n\n\nmed_route\nVARCHAR\neod of medicine delivery\n\n\n\nmed_dose\nVARCHAR\nquantity taken in dose\n\n\n\nmed_dose_unit\nVARCHAR\nunit of dose\n\n\n\n\nExample:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nencounter_id\nmed_order_id\nadmin_dttm\nmed_name\nmed_category\nmed_route\nmed_dose\nmed_dose_unit\n\n\n\n\n2\n43\n10/6/2022 11:10:00\nphenylephrine\nvasoactives\nIntravenous\n0.4\nmcg/kg/min\n\n\n2\n76\n10/6/2022 11:13:00\nphenylephrine\nvasoactives\nIntravenous\n0.75\nmcg/kg/min\n\n\n2\n89\n10/6/2022 11:32:00\ninsulin\nendocrine\nIntravenous\n2\nUnits/hr\n\n\n11\n42\n1/22/2022 00:00:00\npropofol\nsedation\nIntravenous\n40\nmcg/kg/min\n\n\n11\n807\n1/22/2022 02:13:00\npropofol\nsedation\nIntravenous\n30\nmcg/kg/min\n\n\n11\n432\n1/22/2022 04:00:00\nfentanyl\nsedation\nIntravenous\n150\nmcg/hr\n\n\n\n\nNote: The medication_admin_intermittent table has exactly the same schema. The consortium decided to separate the medications that are administered intermittenly from the continuously administered medications."
  },
  {
    "objectID": "projects.html#ongoing-projects-within-the-clif-consortium",
    "href": "projects.html#ongoing-projects-within-the-clif-consortium",
    "title": "CLIF Consortium",
    "section": "",
    "text": "The CLIF consortium is a network of NIH funded investigators.\nPrincipal Investigators (PIs) in the consortium are supported by a wide range of NIH grants that use big data and advanced ML techniques to answer open scientific questions related to critical illness. Each of these projects would be better with CLIF rather than done in silos.\n\n\n\nCLIF Project Description: Creating a life support allocation triage score for crisis standards of care\nGrant: R01 LM014263\nPI: William Parker, University of Chicago\n\n\n\n\n\nCLIF Project Description: Studying hospital variation in low-tidal volume ventilation across hospital systems\nGrant: K23 HL166783\nPI: Nicholas Ingraham, University of Minnesota\n\n\n\n\n\nCLIF Project Description: Validating the association of temperature trajectories and ICU mortality across hospital systems\nGrant: K23 GM144867\nPI: Sivasubramanium Bhavani, Emory University\n\n\n\n\n\nCLIF Project Description: Studying the association between proning practice patterns and ARDS survival\nGrant: K23 HL169743\nPI: Chad Hochberg, John Hopkins University\n\n\n\n\n\nCLIF Project Description: Identifying critically ill patients who are eligible for mobilization using the electronic healthcare record\nGrant: K23 HL148387\nPI: Bhakti Patel, University of Chicago\n\n\n\n\n\nCLIF Project Description: Prediction of severe pneumonia using machine learning\nGrant: K23HL169815\nPI: Catherine Gao, Northwestern University\n\n\n\n\n\nCLIF Project Description: Develop an oncology-specific sepsis prediction model using EHR data and human-centered design\nGrant: K08CA270383\nPI: Patrick Lyons, Oregon Health and Science University"
  },
  {
    "objectID": "about.html#university-of-chicago-1",
    "href": "about.html#university-of-chicago-1",
    "title": "Meet the CLIF Team",
    "section": "University of Chicago",
    "text": "University of Chicago\n\nWilliam Parker\nKaveri Chhikara\nRachel Baccile\nKevin Buell\nKevin Smith\nJay Koyner\nBhakti Patel"
  },
  {
    "objectID": "about.html#principal-investigators",
    "href": "about.html#principal-investigators",
    "title": "Meet the CLIF Team",
    "section": "",
    "text": "William Parker, MD, PhDUniversity of Chicago\n\n\n\n\n\nJ.C. Rojas, MDRush University\n\n\n\n\n\nPat Lyons, MDOregon Health & Science University\n\n\n\n\n\nNicholas Ingraham, MDUniversity of Minnesota\n\n\n\n\n\nChad Hochberg, MDJohn Hopkins University\n\n\n\n\n\nCatherine Gao, MDNorthwestern University\n\n\n\n\n\nSiva Bhavani, MDEmory University\n\n\n\n\n\nSusan Han, MDTufts University\n\n\n\n\n\nKevin Buell, MBBS University of Chicago\n\n\n\n\n\nKevin Smith, MDUniversity of Chicago\n\n\n\n\n\nJay Koyner, MDUniversity of Chicago\n\n\n\n\n\nYuan Luo, PhDNorthwestern University\n\n\n\n\n\nChengsheng Mao, PhDNorthwestern University"
  },
  {
    "objectID": "about.html#data-scientists",
    "href": "about.html#data-scientists",
    "title": "Meet the CLIF Team",
    "section": "Data Scientists",
    "text": "Data Scientists\n\n\n\n\nKaveri Chhikara, MSUniversity of Chicago\n\n\n\n\n\nRachel Baccile, MPPUniversity of Chicago\n\n\n\n\n\nKyle Carey, MSUniversity of Chicago\n\n\n\n\n\nVaishvik C., MSRush University\n\n\n\n\n\nSaki Amagai,PhD StudentNorthwestern University\n\n\n\n\n\nMuna Nour, MPHEmory University"
  },
  {
    "objectID": "about.html#oregon-health-science-university",
    "href": "about.html#oregon-health-science-university",
    "title": "Meet the CLIF Team",
    "section": "Oregon Health & Science University",
    "text": "Oregon Health & Science University\n\n\n\n\nPat Lyons, MD   \n\n\n\n\n\nBrenna Park-Egan, MS"
  },
  {
    "objectID": "about.html#section",
    "href": "about.html#section",
    "title": "Meet the CLIF Team",
    "section": "",
    "text": "Susan Han, MDTufts University"
  },
  {
    "objectID": "data-dictionary.html#provider",
    "href": "data-dictionary.html#provider",
    "title": "CLIF Data Dictionary",
    "section": "Provider",
    "text": "Provider\n\n\n\n\n\n\n\n\nVariable Name\nData Type\nDefinition\n\n\n\n\nencounter_id\nINT\nID variable for each patient encounter\n\n\nprovider_id\nVARCHAR\nID variable for each primary attending\n\n\nstart_dttm\nDATETIME\nStart date and time of the provider (in the format %Y-%m-%d %H:%M:%S)\n\n\nstop_dttm\nDATETIME\nStop date and time of the provider (in the format %Y-%m-%d %H:%M:%S)"
  },
  {
    "objectID": "posts/github-clif/index.html",
    "href": "posts/github-clif/index.html",
    "title": "Using CLIF GitHub with R",
    "section": "",
    "text": "This blog provides a structured approach to setting up GitHub, tailored specifically for our collaborative projects. By standardizing our setup, we can enhance our workflows, ensure consistency across our datasets, and simplify the complexities involved in managing our extensive research data.\n\nStep 1: Prepare your environment\nTo follow this tutorial, you must have a GitHub account and Rstudio installed in your system.\nAdditionally, confirm that GitHub is installed and properly configured on your workstation. Follow the steps on the official GitHub docs\n\n\nStep 2: Fork the repository\nNavigate to the CLIF-1.0 GitHub repository and fork the repository. To fork a repository just means to make a copy that you are going to manage. When you fork a repository you can make changes without affecting the original repository.\nAction:\n\nClick the fork button to create your personal copy of the repository.\nGitHub will create a copy of the repository under your GitHub account.\n\n\n\n\nStep 3: Clone your forked repository\n\nGo to your GitHub account and navigate to the forked repository.\nClick the “Code” button and copy the URL (HTTPS or SSH recommended).\nOpen a terminal (or Git Bash on Windows)\nRun the following command to clone the repository on your local machine. (Replace your-username with your GitHub username) git clone https://github.com/your-username/CLIF-1.0.git\n\n\n\n\nStep 4: Open the repository in RStudio\nNow that you have a local version of the repository on your machine, we can start working! Start by ensuring that Git is configured in Rstudio\n\nConfigure Git in RStudio\n\nOpen RStudio: Launch RStudio on your computer\nGlobal Options: Go to Tools -&gt; Global Options.\nGit/SVN settings:\n\n\nIn the left sidebar, click on Git/SVN.\nEnsure that the “Enable version control interface for RStudio projects” option is checked.\nSet the path to the Git executable. It is usually detected automatically, but if not, you can manually set the path:\n\nOn Windows: It is usually C:/Program Files/Git/bin/git.exe or C:/Program Files (x86)/Git/bin/git.exe.\nOn macOS: It is usually /usr/local/bin/git.\nOn Linux: It is usually /usr/bin/git.\n\n\n\nApply Changes: Click Apply and then click OK to save the settings.\n\n\n\nOpen CLIF repository with Git in RStudio\n\nGo to File -&gt; New Project.\nSelect Existing Directory. This should be the local instance of your forked repository.\nNavigate to the directory of the cloned repository and click Create Project.\n\n\n\nUse the Git tab\nNow you can easily interact with your forked remote repository and the local repository using the below options in RStudio.\n\nCommit Changes:\n\n\nClick on the Git tab.\nYou will see a list of files that have been changed.\nSelect the files you want to commit by checking the boxes next to them.\nClick Commit, enter a commit message, and click Commit again.\n\n\nPush Changes:\n\n\nAfter committing, click the Push button to send your changes to the remote repository.\n\n\nPull Changes:\n\n\nClick the Pull button to fetch and merge changes from the remote repository into your local repository.\n\n\nDiff and History:\n\n\nUse the Diff button to see differences between the current version and the previous commit.\nUse the History button to view the commit history.\n\n\n\n\nStep 5: Pull updates from the original repository\n\nAdd the Original Repository as a Remote:\n\nOpen your project in RStudio.\nGo to the Terminal tab in RStudio (usually located at the bottom left).\nAdd the original repository (upstream) as a remote: git remote add upstream https://github.com/kaveriC/CLIF-1.0.git\n\n\n\nFetch Updates from the Original Repository:\n\nIn the terminal, run git fetch upstream\n\n\n\nMerge Updates into Your Local Branch:\n\nEnsure you are on your main branch (or the branch you want to update): git checkout main\nMerge the changes from the upstream repository into your local branch: git merge upstream/main\nResolve any merge conflicts if they occur.\n\n\n\nPush the Updates to Your Fork:\n\nPush the merged changes to your forked repository: git push origin main\n\n\n\n\nStep 6: Create a Pull Request (PR)\nAfter you commit and push your changes to your forked repository, you can create a pull request to merge these updates with the main original repository.\n\nUsing the GitHub Web Interface for Pull Requests\n\nNavigate to Your Forked Repository: Go to your forked repository on GitHub.\nCreate a Pull Request:\n\nClick on the Pull Requests tab.\nClick the New Pull Request button.\nSelect the base repository (original repository) and the branch you want to merge into (typically main).\nSelect the head repository (your fork) and the branch with your changes.\nClick Create Pull Request.\nAdd a title and description for your pull request, then click Create Pull Request.\n\n\n\n\nUsing RStudio for Pull Requests\nWhile this step is typically done through the GitHub web interface, you can initiate it from the command line if needed.\n\nOpen the terminal in RStudio and use the GitHub CLI (gh) to create a pull request: gh pr create --base kaveriC:main --head your-username:main --title \"Your PR Title\" --body \"Your PR Description\"\nReplace your-username with your GitHub username and customize the title and description as needed."
  },
  {
    "objectID": "about.html#university-of-michigan",
    "href": "about.html#university-of-michigan",
    "title": "Meet the CLIF Team",
    "section": "University of Michigan",
    "text": "University of Michigan\n\n\n\n\nAnna Barker, MD, PhD    \n\n\n\n\n\nMark Nuppnau, MS"
  },
  {
    "objectID": "posts/github-clif copy/index.html",
    "href": "posts/github-clif copy/index.html",
    "title": "EHR to CLIF Microbiology",
    "section": "",
    "text": "This guide provides detailed steps on how to construct the CLIF microbiology table using EHR data."
  },
  {
    "objectID": "posts/github-clif copy/index.html#notes-on-cde-nih-infection-site",
    "href": "posts/github-clif copy/index.html#notes-on-cde-nih-infection-site",
    "title": "EHR to CLIF Microbiology",
    "section": "Notes on CDE NIH Infection Site",
    "text": "Notes on CDE NIH Infection Site\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nCatheter tip\nIncludes central venous catheter tips and pacemaker leads.\n\n\nGI tract unspecified\nUse only if it cannot be categorized into esophagus, stomach, small intestine, or large intestine.\n\n\n\nInterpreted as a luminal GI source.\n\n\nGenito-urinary tract unspecified\nUse for urine cultures.\n\n\n\nIncludes kidney, renal pelvis, ureters, and bladder.\n\n\n\nUse for stents, tissue, abscess, cysts, etc.\n\n\nRash, pustules, or abscesses not typical of any of the above\nInterpreted as a superficial skin pustular source.\n\n\n\nDoes not include deep abscesses (e.g., psoas abscess).\n\n\nLower respiratory tract (lung)\nIncludes all respiratory samples such as tracheal samples, BAL, induced and normal sputum, TBBx, and lung abscesses.\n\n\n\nTypically, fluids are not categorized as “respiratory tract unspecified”.\n\n\n\nRespiratory fluids are complicated to classify; use specimen_description to further classify fluid location (e.g., BAL vs sputum vs trach).\n\n\nJoints\nIncludes samples from within the joint (e.g., synovial, bursa).\n\n\n\nDoes not include overlying skin or muscle.\n\n\nSpinal cord\nDoes not include vertebral osteomyelitis, which falls under “bone cortex (osteomyelitis)”.\n\n\nWound site\nIncludes infected incisions.\n\n\n\nDoes not capture ulcers, as it is often unclear if the ulcer is skin-only vs muscle/fascia vs bone."
  },
  {
    "objectID": "posts/github-clif copy/index.html#note-on-the-cde-organism-table",
    "href": "posts/github-clif copy/index.html#note-on-the-cde-organism-table",
    "title": "EHR to CLIF Microbiology",
    "section": "Note on the CDE Organism table",
    "text": "Note on the CDE Organism table\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nOrganism_name structure\nStructured as “genus_species”.\n\n\n\nIf the species is not available, use “genus_sp”.\n\n\nObligatory anaerobes\nCheck if the bacteria is an obligatory anaerobe.\n\n\n\nClassified into “anaerobic bacteria (nos, except for bacteroides, clostridium)”.\n\n\nGram +/- without genus/species\nFor strings of “gram +/- without genus/species”, use the corresponding categories for fluid_cat.\n\n\n\nApply that category in fluid_name as well."
  },
  {
    "objectID": "posts/microbiology/index.html",
    "href": "posts/microbiology/index.html",
    "title": "CLIF Microbiology",
    "section": "",
    "text": "This guide provides detailed steps on how to construct the CLIF microbiology table using EHR data.\n\n\n\n\n\n\n\n\nencounter_id\ntest_id: For one component (e.g., gram culture smear) for a given fluid with a unique order and collection time. If two pathogens are identified, this yields two rows with the same test_id.\norganism_id: For one culture for a given fluid with a unique order and collection time, linked to a separate sensitivity table. If two pathogens are identified, this yields two rows with the same culture_id.\norder_dttm\ncollect_dttm\nresult_dttm\nfluid_name\nfluid_category (Defined by NIH CDE)\norganism_name\norganism_category (Defined by NIH CDE)\n\n\n\n\n\nCDE NIH Organism: List of categorized organism names defined by NIH.\nclif_vocab_microbiology_organism_ucmc.csv: Helps categorize organism name strings.\nCDE NIH Infection Site: List of categorized infectious fluid names defined by NIH.\nclif_vocab_microbiology_fluid_ucmc.csv: Identifies and categorizes infectious fluid names."
  },
  {
    "objectID": "posts/microbiology/index.html#notes-on-cde-nih-infection-site",
    "href": "posts/microbiology/index.html#notes-on-cde-nih-infection-site",
    "title": "CLIF Microbiology",
    "section": "Notes on CDE NIH Infection Site",
    "text": "Notes on CDE NIH Infection Site\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nCatheter tip\nIncludes central venous catheter tips and pacemaker leads.\n\n\nGI tract unspecified\nUse only if it cannot be categorized into esophagus, stomach, small intestine, or large intestine.\n\n\n\nInterpreted as a luminal GI source.\n\n\nGenito-urinary tract unspecified\nUse for urine cultures.\n\n\n\nIncludes kidney, renal pelvis, ureters, and bladder.\n\n\n\nUse for stents, tissue, abscess, cysts, etc.\n\n\nRash, pustules, or abscesses not typical of any of the above\nInterpreted as a superficial skin pustular source.\n\n\n\nDoes not include deep abscesses (e.g., psoas abscess).\n\n\nLower respiratory tract (lung)\nIncludes all respiratory samples such as tracheal samples, BAL, induced and normal sputum, TBBx, and lung abscesses.\n\n\n\nTypically, fluids are not categorized as “respiratory tract unspecified”.\n\n\n\nRespiratory fluids are complicated to classify; use specimen_description to further classify fluid location (e.g., BAL vs sputum vs trach).\n\n\nJoints\nIncludes samples from within the joint (e.g., synovial, bursa).\n\n\n\nDoes not include overlying skin or muscle.\n\n\nSpinal cord\nDoes not include vertebral osteomyelitis, which falls under “bone cortex (osteomyelitis)”.\n\n\nWound site\nIncludes infected incisions.\n\n\n\nDoes not capture ulcers, as it is often unclear if the ulcer is skin-only vs muscle/fascia vs bone."
  },
  {
    "objectID": "posts/microbiology/index.html#note-on-the-cde-organism-table",
    "href": "posts/microbiology/index.html#note-on-the-cde-organism-table",
    "title": "CLIF Microbiology",
    "section": "Note on the CDE Organism table",
    "text": "Note on the CDE Organism table\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nOrganism_name structure\nStructured as “genus_species”.\n\n\n\nIf the species is not available, use “genus_sp”.\n\n\nObligatory anaerobes\nCheck if the bacteria is an obligatory anaerobe.\n\n\n\nClassified into “anaerobic bacteria (nos, except for bacteroides, clostridium)”.\n\n\nGram +/- without genus/species\nFor strings of “gram +/- without genus/species”, use the corresponding categories for fluid_cat.\n\n\n\nApply that category in fluid_name as well."
  },
  {
    "objectID": "posts/microbiology/index.html#common-problems-encountered",
    "href": "posts/microbiology/index.html#common-problems-encountered",
    "title": "CLIF Microbiology",
    "section": "Common Problems Encountered",
    "text": "Common Problems Encountered\n\nDescribing Organisms:\n\nThere are tens of thousands of strings to describe organisms (e.g., 100,000 CFU E coli, Esch. Coli, E.Coli, etc.).\nUse ontology tables to left_bind and map out organism strings to “organism_name” (genus_species) and “organism_category” (as determined by NIH CDE).\nFor instance, this is a snapshort of 20 records only for negative gram stain and Enterobacter + culture!\n\n\n\n\n\n\nDuplicated Data:\n\nAs one microbiology result can be spread over many rows, left_binding 1:1 using the microbiology ontology tables may create duplicates.\nThese duplicates will need to be addressed later.\n\nNon-descriptive Fluid Names:\n\nSome fluid names describe where the micro fluid is collected (e.g., blood culture anaerobic and aerobic), but many do not (e.g., anaerobic culture, I&D).\nUse other dataset columns, such as “specimen_description” or “source”, to recategorize cultures whose location is not obvious from the “fluid_name”.\nFor instance, At UoC, we have a separate component == “specimen description” that further describes fluid collection. UoW and Northshore also had something similar.\n\n\n\n\n\n\nMultiple Strings for Components:\n\nThere are multiple strings to describe each component of a fluid_name (e.g., gram stain, gram for anaerobic stain, gramstain, etc.).\nDifferent fluid names have components specific to them (e.g., only bacterial cultures have gram stains)."
  },
  {
    "objectID": "posts/microbiology/index.html#step-1-identify-fluid-names-ordering-infectious-studies",
    "href": "posts/microbiology/index.html#step-1-identify-fluid-names-ordering-infectious-studies",
    "title": "CLIF Microbiology",
    "section": "Step 1: Identify Fluid Names Ordering Infectious Studies",
    "text": "Step 1: Identify Fluid Names Ordering Infectious Studies\nTo begin cleaning your CLIF micro data, you first need to identify fluid names that order an infectious study. If your dataset contains all labs ordered at your site, you need to filter out those specific to infectious cultures. The clif_vocab_microbiology_fluid_ucmc table contains a list of infectious fluid names that map to the fluid category. Here’s how you can do it:\nfluid_name_ontology &lt;- read_excel(\"pathway\") %&gt;%\n  mutate(across(where(is.character), tolower)) %&gt;%\n  filter(culture == 1) %&gt;%\n  select(-culture)\nNext, check for other infectious orders that need to be included in your site’s fluid names:\nData %&gt;%\n  filter(is.na(fluid_category)) %&gt;%\n  distinct(fluid_names)\nLook at the table for any infectious fluid names that the micro_ontology table did not pick up. You will need to include these.\nYou can then either:\n\nUpdate the micro_ontology table and repeat the left bind.\nUse case_when(fluid_name == ____) then fluid category is ____."
  },
  {
    "objectID": "posts/microbiology/index.html#introduction",
    "href": "posts/microbiology/index.html#introduction",
    "title": "CLIF Microbiology",
    "section": "",
    "text": "This guide provides detailed steps on how to construct the CLIF microbiology table using EHR data.\n\n\n\n\n\n\n\n\nencounter_id\ntest_id: For one component (e.g., gram culture smear) for a given fluid with a unique order and collection time. If two pathogens are identified, this yields two rows with the same test_id.\norganism_id: For one culture for a given fluid with a unique order and collection time, linked to a separate sensitivity table. If two pathogens are identified, this yields two rows with the same culture_id.\norder_dttm\ncollect_dttm\nresult_dttm\nfluid_name\nfluid_category (Defined by NIH CDE)\norganism_name\norganism_category (Defined by NIH CDE)\n\n\n\n\n\nCDE NIH Organism: List of categorized organism names defined by NIH.\nclif_vocab_microbiology_organism_ucmc.csv: Helps categorize organism name strings.\nCDE NIH Infection Site: List of categorized infectious fluid names defined by NIH.\nclif_vocab_microbiology_fluid_ucmc.csv: Identifies and categorizes infectious fluid names."
  },
  {
    "objectID": "posts/microbiology/index.html#step-2-reclassify-fluid-categories",
    "href": "posts/microbiology/index.html#step-2-reclassify-fluid-categories",
    "title": "CLIF Microbiology",
    "section": "Step 2: Reclassify Fluid Categories",
    "text": "Step 2: Reclassify Fluid Categories\nReclassify fluid_category == “other unspecified” into a more specific category using case_when or grepl. With this approach, about 30% of fluid names might be left bound to the fluid_category \"other unspecified\" because the fluid name does not contain any description of where the fluid originates from.\nReclassify these “other_unspecified” cultures into their correct fluid_category. At UoC, the fluid_description is a string typed in by the clinician in EPIC and is therefore the correct fluid_category.\nIn the below example, we reclassify fluid categories for brain and eyes:\n# Reclassify fluid categories for brain\nsipa_data.brain.pre &lt;- sipa_data.v7 %&gt;%\n  filter(fluid_category == \"other unspecified\" | fluid_category == \"brain\") %&gt;%\n  mutate(fluid_category = case_when(component_name == \"specimen description\" & grepl(\"brain\", ord_value, ignore.case = TRUE) ~ \"brain\",\n                                    TRUE ~ fluid_category)) %&gt;%\n  ungroup()\n\n# Reclassify fluid categories for eyes\nsipa_data.eyes.pre &lt;- sipa_data.v7 %&gt;%\n  filter(fluid_category == \"other unspecified\" | fluid_category == \"eyes\") %&gt;%\n  mutate(fluid_category = case_when(component_name == \"specimen description\" & grepl(\"conjunctival|conjunctivall|corneal|corneal\nEnsure that your reclassification is correct by checking for coherenc. Be careful with your grepl term because it is EXTREMELY easy to miscategorise fluids.\nFor instance, bladder could be gallbladderor urinary bladder\nData %&gt;%\n  mutate(fluid_category = case_when(\n    str_detect(fluid_name, \"bladder\") ~ \"urinary bladder\",\n    TRUE ~ fluid_category\n  ))\nAfter every grepl/case_when, you need to\nData %&gt;%\n  filter(fluid_category == \"fluid_category\") %&gt;%\n  distinct(fluid_names, fluid_category)\nLook at the table to see that the pairings are coherent."
  },
  {
    "objectID": "posts/microbiology/index.html#step-2-check-for-other-infectious-orders",
    "href": "posts/microbiology/index.html#step-2-check-for-other-infectious-orders",
    "title": "CLIF Microbiology",
    "section": "Step 2: Check for other infectious orders",
    "text": "Step 2: Check for other infectious orders\nNext, check for other infectious orders that need to be included in your site’s fluid names:\nData %&gt;%\n  filter(is.na(fluid_category)) %&gt;%\n  distinct(fluid_names)\nLook at the table for any infectious fluid names that the micro_ontology table did not pick up. You will need to include these.\nYou can then either:\n\nUpdate the micro_ontology table and repeat the left bind.\nUse case_when(fluid_name == ____) then fluid category is ____."
  },
  {
    "objectID": "posts/microbiology/index.html#step-3-reclassify-fluid-categories",
    "href": "posts/microbiology/index.html#step-3-reclassify-fluid-categories",
    "title": "CLIF Microbiology",
    "section": "Step 3: Reclassify Fluid Categories",
    "text": "Step 3: Reclassify Fluid Categories\nReclassify fluid_category == “other unspecified” into a more specific category using case_when or grepl. With this approach, about 30% of fluid names might be left bound to the fluid_category \"other unspecified\" because the fluid name does not contain any description of where the fluid originates from.\nReclassify these “other_unspecified” cultures into their correct fluid_category. At UoC, the fluid_description is a string typed in by the clinician in EPIC and is therefore the correct fluid_category.\nData %&gt;%\n  mutate(fluid_category = case_when(\n    str_detect(fluid_name, \"bladder\") ~ \"urinary bladder\",\n    TRUE ~ fluid_category\n  ))\nEnsure that your reclassification is correct by checking for coherence:\nData %&gt;%\n  filter(fluid_category == \"fluid_category\") %&gt;%\n  distinct(fluid_names, fluid_category)"
  },
  {
    "objectID": "posts/microbiology/index.html#step-3-create-component-categories",
    "href": "posts/microbiology/index.html#step-3-create-component-categories",
    "title": "CLIF Microbiology",
    "section": "Step 3: Create Component categories",
    "text": "Step 3: Create Component categories\nThe type of fluid name should be specified. There are three levels for this: Culture, Gram stain, and Smear.\nSince there were few component names at UoC, an ontology table wasn’t created. Instead, you can use case_when to create a new column for component_category:\nData %&gt;%\n  mutate(component_category = case_when(\n    str_detect(component_name, \"gram stain\") ~ \"Gram stain\",\n    str_detect(component_name, \"smear\") ~ \"Smear\",\n    TRUE ~ \"Culture\"\n  ))"
  },
  {
    "objectID": "posts/microbiology/index.html#step-4-categorize-organism-name-strings",
    "href": "posts/microbiology/index.html#step-4-categorize-organism-name-strings",
    "title": "CLIF Microbiology",
    "section": "Step 4: Categorize Organism Name Strings",
    "text": "Step 4: Categorize Organism Name Strings\nCategorize organism name strings into organism_name and organism_category by left binding the clif_vocab_microbiology_organism_ucmc.csv table:\n\n\n\n\n\nData &lt;- Data %&gt;%\n  left_join(micro_name_ontology, by = \"organism_string\")\nCheck for organism strings that did not get captured with the left bind:\nData %&gt;%\n  filter(is.na(organism_category)) %&gt;%\n  distinct(organism_string)\nInclude any missing organism strings by either:\n\nUsing case_when for specific strings.\nUpdating the organism_name_ontology table manually and repeating the left bind.\n\nFor updating the organism name ontology table, you could also use the CLIF Assitant. You can feed CLIF assistant your strings. It can help you classify them into organism_names and organism_categories."
  },
  {
    "objectID": "posts/microbiology/index.html#step-5-final-steps-to-clean-the-data",
    "href": "posts/microbiology/index.html#step-5-final-steps-to-clean-the-data",
    "title": "CLIF Microbiology",
    "section": "Step 5: Final Steps to Clean the Data",
    "text": "Step 5: Final Steps to Clean the Data\n\nCreate Test ID and Culture ID\nA test_id is a unique number that identifies one patient, one infectious lab (fluid_name) collected at one time point, and its component_name.\nA culture_id is a unique number that identifies one patient, one infectious lab (fluid_name) collected at one time point, and its component_name for a culture."
  },
  {
    "objectID": "posts/microbiology/index.html#potential-additional-steps",
    "href": "posts/microbiology/index.html#potential-additional-steps",
    "title": "CLIF Microbiology",
    "section": "Potential additional steps",
    "text": "Potential additional steps\nWhether or not you need to perform these additional steps depends on how many redundant string rows your dataset has\n\nRemove Duplicate Rows\n\n\nTo remove duplicate rows, use the following R code:\n\ndistinct(encounter_id, order_dttm, collect_dttm, result_dttm, fluid_name, fluid_category, component_name, component_category, organism_name, organism_category, susceptibility_columns)\n\nOne organism collected from a patient, from one body fluid, at one time, growing one bug, with a set susceptibility pattern should take up one row\n\nIf two different bugs grow from same culture, they take up two rows\nIf two identical bugs grow with different susceptibility profile, they take up two rows\n\n\n\nReview additional rows of\n\n\nNo growth\n\nExample that no growth should be removed. The below screenshot is one culture. The “no growth” of “no bacteroides fragilis group or clostridium perfringes isolated” comes from left bind. It is not appropriate given the culture actually grows pseudomonas.\n\n\n\n\n\n\nOther bacteria\n\nExample that other bacteria should be removed. We ultimately want one row showing the growth enterococcus and not “other bacteria”\n\n\n\n\n\n\n\n\nNotes on CDE NIH Infection Site\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nCatheter tip\nIncludes central venous catheter tips and pacemaker leads.\n\n\nGI tract unspecified\nUse only if it cannot be categorized into esophagus, stomach, small intestine, or large intestine.\n\n\n\nInterpreted as a luminal GI source.\n\n\nGenito-urinary tract unspecified\nUse for urine cultures.\n\n\n\nIncludes kidney, renal pelvis, ureters, and bladder.\n\n\n\nUse for stents, tissue, abscess, cysts, etc.\n\n\nRash, pustules, or abscesses not typical of any of the above\nInterpreted as a superficial skin pustular source.\n\n\n\nDoes not include deep abscesses (e.g., psoas abscess).\n\n\nLower respiratory tract (lung)\nIncludes all respiratory samples such as tracheal samples, BAL, induced and normal sputum, TBBx, and lung abscesses.\n\n\n\nTypically, fluids are not categorized as “respiratory tract unspecified”.\n\n\n\nRespiratory fluids are complicated to classify; use specimen_description to further classify fluid location (e.g., BAL vs sputum vs trach).\n\n\nJoints\nIncludes samples from within the joint (e.g., synovial, bursa).\n\n\n\nDoes not include overlying skin or muscle.\n\n\nSpinal cord\nDoes not include vertebral osteomyelitis, which falls under “bone cortex (osteomyelitis)”.\n\n\nWound site\nIncludes infected incisions.\n\n\n\nDoes not capture ulcers, as it is often unclear if the ulcer is skin-only vs muscle/fascia vs bone.\n\n\n\n\n\nNotes on the CDE Organism table\n\n\n\n\n\n\n\nCategory\nDetails\n\n\n\n\nOrganism_name structure\nStructured as “genus_species”.\n\n\n\nIf the species is not available, use “genus_sp”.\n\n\nObligatory anaerobes\nCheck if the bacteria is an obligatory anaerobe.\n\n\n\nClassified into “anaerobic bacteria (nos, except for bacteroides, clostridium)”.\n\n\nGram +/- without genus/species\nFor strings of “gram +/- without genus/species”, use the corresponding categories for fluid_cat.\n\n\n\nApply that category in fluid_name as well.\n\n\n\n\n\nCommon Problems Encountered\n\nDescribing Organisms:\n\nThere are tens of thousands of strings to describe organisms (e.g., 100,000 CFU E coli, Esch. Coli, E.Coli, etc.).\nUse ontology tables to left_bind and map out organism strings to “organism_name” (genus_species) and “organism_category” (as determined by NIH CDE).\nFor instance, this is a snapshort of 20 records only for negative gram stain and Enterobacter + culture!\n\n\n\n\n\n\nDuplicated Data:\n\nAs one microbiology result can be spread over many rows, left_binding 1:1 using the microbiology ontology tables may create duplicates.\nThese duplicates will need to be addressed later.\n\nNon-descriptive Fluid Names:\n\nSome fluid names describe where the micro fluid is collected (e.g., blood culture anaerobic and aerobic), but many do not (e.g., anaerobic culture, I&D).\nUse other dataset columns, such as “specimen_description” or “source”, to recategorize cultures whose location is not obvious from the “fluid_name”.\nFor instance, At UoC, we have a separate component == “specimen description” that further describes fluid collection. UoW and Northshore also had something similar.\n\n\n\n\n\n\nMultiple Strings for Components:\n\nThere are multiple strings to describe each component of a fluid_name (e.g., gram stain, gram for anaerobic stain, gramstain, etc.).\nDifferent fluid names have components specific to them (e.g., only bacterial cultures have gram stains)."
  },
  {
    "objectID": "data-dictionary.html#prone",
    "href": "data-dictionary.html#prone",
    "title": "CLIF Data Dictionary",
    "section": "Prone",
    "text": "Prone"
  }
]
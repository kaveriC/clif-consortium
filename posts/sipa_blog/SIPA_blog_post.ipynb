{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Constructing an Analytic Dataset from CLIF\"\n",
    "author: \"Rachel Baccile\"\n",
    "date: \"2024-2-7\"\n",
    "categories: [longitudinal-dataset, analysis, SIPA]\n",
    "tbl-colwidths: [75,25]\n",
    "format:\n",
    "  html:\n",
    "    theme: sandstone\n",
    "    css: styles.css  #\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cohort Identification\n",
    "**Cohort Definition**: Data from UCMC COVID Datamart. Adult patients (18+) admitted January 1 2020 â€“ March 31, 2022 and who recieved life support. Life support was defined as:\n",
    "\n",
    "- Received vasoactive medications for shock, **or**\n",
    "- Received invasive or non-invasive mechanical ventilation, **or**\n",
    "- Received oxygen therapy with PaO2/FiO2 < 200 (S/F < 179 if no P/F measured)\n",
    "\n",
    "Patients were excluded in they were discharged from the ED (including to hospice) and if they received < 6 hours of life support.\n",
    "\n",
    "**CLIF tables used**:\n",
    "\n",
    "- RCLIF_limited_identifers (admission date, discharge date)\n",
    "- RCLIF_encounter_demographics_dispo (age at admission, disposition)\n",
    "- RCLIF_adt (location in hospital)\n",
    "- RCLIF_resp_support (respiratory support device, FiO2)\n",
    "- RCLIF_labs (PaO2)\n",
    "- RCLIF_vitals (SpO2)\n",
    "- RCLIF_medication_adm_continuous (vasopressors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Identify Adults during time period that were admitted to the hospital\n",
    "1. Use RCLIF_limited_identifers to identify admissions in the time period of interest (variable: admission_dttm)\n",
    "2. Use RCLIF_encounter_demographics_dispo to filter for only those 18+ at time of admission (variable: age_at_admission)\n",
    "3. Use the ADT table to filter out those only seen in the ER (variable: dept_name)\n",
    "4. \"Explode\" the data between admission_dttm and discharge_dttm to get hourly blocks for the duration of admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "######## Load in limited IDs for admission dates in time period\n",
    "limited_ids = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_limited_identifers.csv\")\n",
    "limited_ids = limited_ids.withColumn('admission_datetime',f.to_timestamp('admission_date','yyyy-MM-dd HH:mm:ss'))\n",
    "limited_ids = limited_ids.withColumn('discharge_datetime',f.to_timestamp('discharge_date','yyyy-MM-dd HH:mm:ss'))\n",
    "limited_ids = limited_ids.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime', 'zip_code').distinct()\n",
    "\n",
    "# Filter to time period\n",
    "limited_ids = limited_ids.filter(((f.col('admission_datetime')>='2020-03-01') & \n",
    "                   (f.col('admission_datetime')<='2022-03-31')))\n",
    "limited_ids = limited_ids.filter((f.col('discharge_datetime')>=f.col('admission_datetime')))\n",
    "limited_ids = limited_ids.filter(f.col('discharge_datetime').isNotNull())\n",
    "limited_ids = limited_ids.filter(f.col('admission_datetime').isNotNull())\n",
    "\n",
    "\n",
    "######## Load in encounters for age and discharge disposition\n",
    "demo_disp = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_encounter_demographics_dispo.csv\")\n",
    "demo_disp = demo_disp.withColumn(\"age_at_admission\",demo_disp.age_at_admission.cast('double'))\n",
    "demo_disp = demo_disp.select('C19_PATIENT_ID', 'C19_HAR_ID', 'age_at_admission', 'disposition')\n",
    "\n",
    "# Filter to adults only\n",
    "demo_disp = demo_disp.filter(f.col('age_at_admission')>=18)\n",
    "\n",
    "adults_in_time = limited_ids.join(demo_disp, on='C19_HAR_ID', how='inner')\n",
    "\n",
    "# Exclude people only in ER\n",
    "adt = spark.read.option(\"header\",True).csv(\"/project2/wparker/SIPA_data/RCLIF_adt.csv\")\n",
    "\n",
    "adult_ids = adults_in_time.select('C19_HAR_ID')\n",
    "adt_adults = adult_ids.join(adt, on='C19_HAR_ID', how='inner')\n",
    "adt_adults = adt_adults.select('C19_HAR_ID', 'dept_name').distinct()\n",
    "adt_adults = adt_adults.withColumn('count', f.lit(1))\n",
    "adt_adults_wide = adt_adults.groupBy('C19_HAR_ID').pivot('dept_name').agg(f.sum('count'))\n",
    "adt_adults_wide = adt_adults_wide.filter(f.col('ICU').isNotNull() |\n",
    "                                         f.col('NA').isNotNull() |\n",
    "                                         f.col('OR').isNotNull() |\n",
    "                                         f.col('Ward').isNotNull())\n",
    "adt_adults_wide = adt_adults_wide.select('C19_HAR_ID').distinct()\n",
    "\n",
    "adults_in_time = adt_adults_wide.join(adults_in_time, on='C19_HAR_ID', how='left')\n",
    "har_ids = adults_in_time.select('C19_HAR_ID', 'admission_datetime', 'discharge_datetime')\n",
    "\n",
    "## Explode between admission and discharge to get all hourly timestamps\n",
    "har_ids_hours = har_ids.withColumn('txnDt', f.explode(f.expr('sequence(admission_datetime, discharge_datetime, interval 1 hour)')))\n",
    "har_ids_hours = har_ids_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\n",
    "har_ids_hours = har_ids_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\n",
    "har_ids_hours = har_ids_hours.select('C19_HAR_ID', 'txnDt', 'meas_date', 'meas_hour').orderBy('C19_HAR_ID', 'txnDt')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Determine if patients meet respiratory criteria\n",
    "Use RCLIF_resp_support table to identify instances of invasive or non-invasive mechanical ventilation. If no invasive or non-invasive mechanical ventilation, use RCLIF_resp_support, RCLIF_labs, and RCLIF_vitals to calculate PaO2/FiO2 or Sp02."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Prepare RCLIF_resp_support\n",
    "1. Inner join RCLIF_resp_support to adults hospitalized in time period, identified in previous code block.\n",
    "2. Filter out invalid FiO2 measurements\n",
    "    - N removed: 3,440 observations; 376 unique encounters\n",
    "3. Remove instances of CPAP\n",
    "    - CPAP defined as device_name == \"NIPPV\" and without an FiO2 measurement, or when mode_name == \"CPAP\"\n",
    "    - N removed: 357,851 observations; 128 unique encounters\n",
    "4. If device_name was missing, fill in based on the following logic:\n",
    "    - If fio2 ==.21 and lpm, peep, and set_volume are null, then device_name == Room Air\n",
    "    - If fio2 ==.21, lpm==0, and peep and set_volume are null, then device_name == Room Air\n",
    "    - If fio2, peep, and set_volume are null and lpm <=20 then device_name == Nasal Cannula\n",
    "    - If fio2, peep, and set_volume are null and lpm >20 then device_name == High Flow NC\n",
    "    - If fio2 and lpm are null and set_volumne is not null, then device_name == Vent\n",
    "5. If device_name was Nasal Cannula but lpm > 20, then device_name== High Flow NC\n",
    "6. If fio2 is missing and device_name == Room Air, then fio2== 0.21\n",
    "7. If fio2 is missing and device_name == Nasal Cannula, then fio2 was calculated as: ( 0.24 + (0.04 * lpm) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "######### Get worst FiO2\n",
    "\n",
    "## Read in respiratory support table\n",
    "resp_full = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_resp_support.csv')\n",
    "resp_full = resp_full.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "resp_full = resp_full.withColumn(\"fio2\",resp_full.fio2.cast('double'))\n",
    "resp_full = resp_full.withColumn(\"lpm\",resp_full.lpm.cast('double'))\n",
    "resp_full = resp_full.withColumn(\"peep\",resp_full.peep.cast('double'))\n",
    "resp_full = resp_full.withColumn(\"set_volume\",resp_full.peep.cast('double'))\n",
    "\n",
    "\n",
    "## Filter to only adults in time frame\n",
    "resp_full = har_ids.join(resp_full, on='C19_HAR_ID', how='inner')\n",
    "##Filter to only variables we need\n",
    "resp_full = resp_full.select('C19_PATIENT_ID', 'C19_HAR_ID', 'recorded_time', 'device_name', 'mode_name', 'mode_category','lpm', 'fio2', 'peep', 'set_volume')\n",
    "\n",
    "## Filter for valid values or Null\n",
    "resp_full = resp_full.filter((((f.col('fio2')>=0.21) &\n",
    "                              (f.col('fio2')<=1)) |\n",
    "                              (f.col('fio2').isNull())))\n",
    "resp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n",
    "\n",
    "## Filter out people on NIPPV without a FiO2 measurement--CPAP\n",
    "nippv_test = resp_full.filter(f.col('device_name')==\"NIPPV\")\n",
    "nippv_test.summary().show()\n",
    "\n",
    "resp_full = resp_full.filter(~((f.col('device_name')=='NIPPV') &\n",
    "                              (f.col('fio2').isNull())&\n",
    "                              (f.col('lpm').isNull()) &\n",
    "                              (f.col('peep').isNull())))\n",
    "\n",
    "resp_full = resp_full.filter(~f.col('mode_name').rlike(r'CPAP'))\n",
    "resp_full.select(f.countDistinct(\"C19_PATIENT_ID\")).show()\n",
    "\n",
    "## Replace \"NA\" strings with actual Nulls\n",
    "resp_full.select('device_name').distinct().show()\n",
    "resp_full = resp_full.withColumn('device_name', f.when(~f.col('device_name').rlike(r'NA'), f.col('device_name')))\n",
    "\n",
    "## Try to fill in some of the null device names based on other values\n",
    "resp_full = resp_full.withColumn('device_name_2', f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN device_name IS NOT NULL THEN device_name\n",
    "        WHEN device_name IS NULL AND fio2 ==.21 AND lpm IS NULL AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n",
    "        WHEN device_name IS NULL AND fio2 IS NULL AND lpm ==0 AND peep IS NULL AND set_volume IS NULL THEN 'Room Air'\n",
    "        WHEN device_name IS NULL AND fio2 IS NULL AND lpm <=20 AND peep IS NULL AND set_volume IS NULL THEN 'Nasal Cannula'\n",
    "        WHEN device_name IS NULL AND fio2 IS NULL AND lpm >20 AND peep IS NULL AND set_volume IS NULL THEN 'High Flow NC'\n",
    "        WHEN device_name IS NULL AND fio2 IS NULL AND lpm IS NULL AND set_volume IS NOT NULL THEN 'Vent'\n",
    "        WHEN device_name == \"Nasal Cannula\" AND fio2 IS NULL AND lpm >20 THEN 'High Flow NC'\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "))\n",
    "\n",
    "## Try to fill in FiO2 based on LPM for nasal cannula\n",
    "resp_full = resp_full.withColumn('fio2_combined', f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2 IS NOT NULL THEN fio2\n",
    "        WHEN fio2 IS NULL AND device_name_2 == 'Room Air' THEN .21\n",
    "        WHEN fio2 IS NULL AND device_name_2 == 'Nasal Cannula' THEN ( 0.24 + (0.04 * lpm) )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Creating Respiratory Support Hourly Blocking\n",
    "**Goal**: For each one hour interval, identify highest level of respiratory support and maximum FiO2.\n",
    "\n",
    "1. Extract the measurement hour and measurement date from the recorded_time\n",
    "2. Rank respiratory support devices\n",
    "3. Group by C19_HAR_ID, measurement date, and measurement hour, then take the minimum of device_rank (highest level of support) and maximum of fio2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Extract hour and date for blocking\n",
    "resp_full = resp_full.select('C19_HAR_ID', 'device_name_2', 'recorded_time', 'fio2_combined')\n",
    "resp_full = resp_full.withColumn('meas_hour', f.hour(f.col('recorded_time')))\n",
    "resp_full = resp_full.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n",
    "\n",
    "fio2 = resp_full.select('C19_HAR_ID', 'device_name_2', 'meas_date', 'meas_hour', 'fio2_combined').distinct()\n",
    "\n",
    "## Need to rank devices to get max in hour\n",
    "fio2 = fio2.withColumn(\"device_rank\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN device_name_2 == 'Vent' THEN 1\n",
    "        WHEN device_name_2 == 'NIPPV' THEN 2\n",
    "        WHEN device_name_2 == 'High Flow NC' THEN 3\n",
    "        WHEN device_name_2 == 'Face Mask' THEN 4 \n",
    "        WHEN device_name_2 == 'Trach Collar' THEN 5\n",
    "        WHEN device_name_2 == 'Nasal Cannula' THEN 6 \n",
    "        WHEN device_name_2 == 'Other' THEN 7\n",
    "        WHEN device_name_2 == 'Room Air' THEN 8\n",
    "        WHEN device_name_2 IS NULL THEN NULL\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "## Group by person, device, measurement date and measurement hour; get max FiO2 and LPM within each hour\n",
    "group_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\n",
    "fio2 = fio2.groupBy(group_cols) \\\n",
    "            .agg((f.max('fio2_combined').alias(\"fio2_combined\")),\n",
    "                  (f.min('device_rank').alias(\"device_rank\"))).orderBy(group_cols)\n",
    "fio2 = fio2.withColumn(\"device_name\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN device_rank == 1 THEN 'Vent'\n",
    "        WHEN device_rank == 2 THEN 'NIPPV' \n",
    "        WHEN device_rank == 3 THEN 'High Flow NC'\n",
    "        WHEN device_rank == 4 THEN 'Face Mask' \n",
    "        WHEN device_rank == 5 THEN 'Trach Collar'\n",
    "        WHEN device_rank == 6 THEN 'Nasal Cannula'\n",
    "        WHEN device_rank == 7 THEN 'Other'\n",
    "        WHEN device_rank == 8 THEN 'Room Air'\n",
    "        WHEN device_rank IS NULL THEN NULL\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a dataset with one observation per C19_HAR_ID per hour, with the values representing their worst respiratory state in that hour, but only for the hours that have a recorded value. For example, if someone is on a Nasal Cannula with a constant setting for many hours, only the hour that they were started on that device may have an entry. Therefore, we need to carry forward values until another value is recorded, indicating a change in respiratory support, discharge, or death. This will allow us to merge in the minimum PaO2 values per hour using the RCLIF_labs table (see section #) and ensure the PaO2 and FiO2 were measured within the same hour when calculating the PaO2/FiO2 ratio.\n",
    "\n",
    "To do this, we:\n",
    "1. Left join back to hourly blocked dataset created in previous code chunk\n",
    "2. Carry forward values until a change or until discharge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Merge back to hourly blocked cohort table \n",
    "group_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\n",
    "fio2_hours = har_ids_hours.join(fio2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n",
    "\n",
    "\n",
    "## Carry forward device name until another device is recorded or the end of the measurement time window\n",
    "fio2_hours = fio2_hours.withColumn('device_filled', \n",
    "                                       f.coalesce(f.col('device_name'), \n",
    "                                                  f.last('device_name', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy('txnDt')), f.lit('NULL')))\n",
    "fio2_hours = fio2_hours.withColumn('device_filled', \n",
    "                                       f.when(~f.col('device_filled').rlike(r'NULL'), f.col('device_filled')))\n",
    "\n",
    "## Carry forward FiO2 measurement name until another device is recorded or the end of the measurement time window\n",
    "fio2_hours = fio2_hours.withColumn(\"fio2_combined\",fio2_hours.fio2_combined.cast('double'))\n",
    "\n",
    "fio2_hours = fio2_hours.withColumn('fio2_filled', \n",
    "                                       f.when((f.col('fio2_combined').isNotNull()), f.col('fio2_combined')))\n",
    "fio2_hours = fio2_hours.withColumn('fio2_filled', \n",
    "                                       f.coalesce(f.col('fio2_combined'), \n",
    "                                                  f.last('fio2_combined', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID', 'device_filled') \\\n",
    "                                                        .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "fio2_filled = fio2_hours.select('C19_HAR_ID','txnDt','meas_date', 'meas_hour',\n",
    "                                  'device_filled','fio2_filled').distinct()\n",
    "                                  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to bring in PaO2 values using the RCLIF_labs table to calculate PaO2/FiO2 ratio. We will conduct a similar process as above to create an hourly blocked dataset of the minimum PaO2 per hour.\n",
    "\n",
    "1. Inner join RCLIF_labs to adults hospitalized in time period\n",
    "2. Filter to just PaO2 labs\n",
    "3. Group by C19_HAR_ID, measurement date, and measurement hour, then take the minimum lab_value\n",
    "4. Left join back to hourly blocked dataset\n",
    "5. Carry forward values for only 4 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Now need PaO2\n",
    "labs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\n",
    "\n",
    "### Selecting only variables we need\n",
    "labs = labs.select('C19_HAR_ID', 'lab_result_time','lab_name', 'lab_value')\n",
    "\n",
    "### Filtering to only adults in time period\n",
    "labs = labs.join(har_ids, on='C19_HAR_ID', how='inner')\n",
    "\n",
    "### Filtering to only PaO2, formatting values\n",
    "labs = labs.filter(f.col(\"lab_name\")==\"pao2\")\n",
    "labs = labs.withColumn(\"lab_value\",labs.lab_value.cast('double'))\n",
    "labs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Get min PaO2 per hour\n",
    "labs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\n",
    "labs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\n",
    "pao2 = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "pao2 = pao2.groupBy(group_cols) \\\n",
    "           .pivot(\"lab_name\") \\\n",
    "           .agg(f.min('lab_value').alias(\"min\")).orderBy(group_cols)\n",
    "\n",
    "\n",
    "## Merge back to hourly blocked cohort table \n",
    "group_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\n",
    "pao2_hours = har_ids_hours.join(pao2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n",
    "\n",
    "\n",
    "## Carry forward PaO2 until next measurement or end of window, maximum 4 hours\n",
    "\n",
    "### Get time of most recent PaO2\n",
    "pao2_hours = pao2_hours.withColumn('last_measure', f.when(f.col('pao2').isNotNull(), f.col('txnDt')))\n",
    "pao2_hours = pao2_hours.withColumn('last_measure', f.coalesce(f.col('last_measure'), \n",
    "                                                                  f.last('last_measure', True)\\\n",
    "                                                                  .over(Window.partitionBy('C19_HAR_ID')\\\n",
    "                                                                        .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "pao2_hours = pao2_hours.withColumn('last_measure',f.to_timestamp('last_measure','yyyy-MM-dd HH:mm:ss'))\n",
    "pao2_hours = pao2_hours.withColumn('txnDt',f.to_timestamp('txnDt','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "### Calculate time difference between the hour we're trying to fill and the most recent PaO2, filter to only 3 additional hrs (4 total)\n",
    "pao2_hours = pao2_hours.withColumn(\"hour_diff\", \n",
    "                                       (f.col(\"txnDt\").cast(\"long\")-f.col(\"last_measure\").cast(\"long\"))/(60*60))\n",
    "pao2_hours_2 = pao2_hours.filter((f.col('hour_diff')>=0)&(f.col('hour_diff')<=3))\n",
    "\n",
    "### Fill PaO2 forward\n",
    "pao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.when(f.col('pao2').isNotNull(), f.col('pao2')))\n",
    "pao2_hours_2 = pao2_hours_2.withColumn('pao2_filled', f.coalesce(f.col('pao2'), \n",
    "                                                                 f.last('pao2', True)\\\n",
    "                                                                 .over(Window.partitionBy('C19_HAR_ID', \n",
    "                                                                                          'last_measure')\\\n",
    "                                                                       .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "pao2_filled = pao2_hours_2.select('C19_HAR_ID','meas_date', 'meas_hour', 'pao2_filled')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we repeat the process with the RCLIF_vitals table to obtain SpO2 measurements. In the absence of a PaO2/FiO2 ratio, we use SpO2/FiO2 ratio to determine cohort inclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Now need spO2\n",
    "vitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals.parquet\")\n",
    "\n",
    "### Filtering to only adults in time period\n",
    "vitals = vitals.join(har_ids, on='C19_HAR_ID', how='inner')\n",
    "\n",
    "### Formatting values\n",
    "vitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "vitals = vitals.withColumn(\"vital_value\",vitals.vital_value.cast('double'))\n",
    "\n",
    "### Selecting variables we need\n",
    "vitals = vitals.select('C19_HAR_ID', 'measured_time','vital_name', 'vital_value')\n",
    "vitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\n",
    "vitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\n",
    "\n",
    "### Filtering to only SpO2, valid values\n",
    "spo2 = vitals.filter(f.col(\"vital_name\")==\"spO2\")\n",
    "spo2 = spo2.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'vital_name', 'vital_value')\n",
    "spo2 = spo2.filter(f.col('vital_value')>60)\n",
    "spo2 = spo2.filter(f.col('vital_value')<=100)\n",
    "\n",
    "# Get min SpO2 per hour\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "spo2 = spo2.groupBy(group_cols) \\\n",
    "           .pivot(\"vital_name\") \\\n",
    "           .agg(f.min('vital_value').alias(\"min\"))\n",
    "\n",
    "## Merge back to hourly blocked cohort table \n",
    "group_cols = [\"C19_HAR_ID\", \"meas_date\", \"meas_hour\"]\n",
    "spo2_hours = har_ids_hours.join(spo2, on=group_cols, how='left').orderBy('C19_HAR_ID', 'txnDt').distinct()\n",
    "\n",
    "\n",
    "## Cary forward SpO2\n",
    "spo2_hours = spo2_hours.withColumn('spO2_filled', \n",
    "                                           f.when(f.col('spO2').isNotNull(), f.col('spO2')))\n",
    "spo2_hours = spo2_hours.withColumn('spO2_filled', \n",
    "                                           f.coalesce(f.col('spO2'), \n",
    "                                                      f.last('spO2', True)\\\n",
    "                                                      .over(Window.partitionBy('C19_HAR_ID', 'last_measure')\\\n",
    "                                                            .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "spO2_filled = spo2_hours.select('C19_HAR_ID','meas_date', 'meas_hour', 'spO2_filled')\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have hourly blocked respiratory support, FiO2, PaO2, and SpO2, we can full join these tables to determine the first hour a patient met the respiratory inclusion criteria (the minimum recorded time of either FiO2/PaO2 < 200, SpO2/FiO2 <179, non-invasive ventilation, or invasive ventilation). I also write this hourly blocked dataset for later use in the 48-hour analytic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "# Merge FiO2, PaO2, spO2 to get FiO2/PaO2\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\",\"meas_date\", \"meas_hour\"]\n",
    "df = fio2_filled.join(pao2_filled, on=group_cols, how='full')\n",
    "df = df.join(spO2_filled, on=group_cols, how='full').orderBy(group_cols)\n",
    "\n",
    "df = df.withColumn(\"fio2_filled\",df.fio2_filled.cast('double'))\n",
    "df = df.withColumn(\"pao2_filled\",df.pao2_filled.cast('double'))\n",
    "df = df.withColumn(\"spO2_filled\",df.spO2_filled.cast('double'))\n",
    "\n",
    "# Get first time on oxygen support & P/F <200\n",
    "df = df.withColumn(\"p_f\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2_filled IS NOT NULL AND pao2_filled IS NOT NULL THEN ( pao2_filled / fio2_filled )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "df = df.withColumn(\"s_f\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN fio2_filled IS NOT NULL AND spO2_filled IS NOT NULL THEN ( spO2_filled / fio2_filled )\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "df = df.distinct()\n",
    "df.write.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\", mode=\"overwrite\")\n",
    "\n",
    "## Get first time somone on oxygen therapy with PaO2/FiO2 < 200 (S/F < 179 if no P/F measured) and invasive and non-invasive ventilation\n",
    "\n",
    "df = df.filter((((f.col(\"p_f\")<200))|\n",
    "                (f.col(\"s_f\")<179) |\n",
    "                (f.col('device_filled')=='Vent') | \n",
    "                (f.col('device_filled')=='NIPPV')))\n",
    "\n",
    "\n",
    "df = df.select(\"C19_HAR_ID\", \"txnDt\", \"device_filled\",\"pao2_filled\",\"fio2_filled\", \"spO2_filled\", \"p_f\", \"s_f\")\n",
    "\n",
    "w1 = Window.partitionBy(\"C19_HAR_ID\").orderBy('txnDt')\n",
    "\n",
    "df_first_with_time = df.withColumn(\"row\",f.row_number().over(w1)) \\\n",
    "             .filter(f.col(\"row\") == 1).drop(\"row\")\n",
    "\n",
    "df_first_with_time = df_first_with_time.select(\"C19_HAR_ID\", \"txnDt\").withColumnRenamed(\"txnDt\", \"recorded_time\")\n",
    "\n",
    "resp_support = df_first_with_time.groupBy(\"C19_HAR_ID\").agg(f.min(\"recorded_time\").alias(\"resp_life_support_start\")).distinct()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Determine if patients received vasopressors\n",
    "\n",
    "Using the RCLIF_medication_admin_continuous table, we identify the first instance a patient is started on a vasopressor. We inner join the adults hospitalized in the time period to the RCLIF_medication_admin_continuous table, then filter to only vasopressor medications, then take the first time per patient per encounter to use as the index time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# Now pressors\n",
    "df_meds = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_meds_admin_conti.csv')\n",
    "df_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Filter to pressor medications\n",
    "\n",
    "pressors = df_meds.filter(f.col('med_category')=='vasoactives')\n",
    "pressors = pressors.select(\"C19_HAR_ID\", \"admin_time\")\n",
    "\n",
    "# Get first time someone is on a pressor\n",
    "pressors = pressors.groupBy(\"C19_HAR_ID\").agg(f.min(\"admin_time\").alias(\"pressor_life_support_start\"))\n",
    "pressors = pressors.join(har_ids, on='C19_HAR_ID', how='inner').distinct()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Determine start of life support\n",
    "\n",
    "The start of life support will be the index time for constructing the 48 hour analytic dataset (41 hours before, the index hour, and 6 hours after). To do that, we take the minimum between the time respiratory criteria was met and the time vasopressors were started. Now we have a cohort identified with the first hour block life support was initiated (index time). \n",
    "\n",
    "We also calculate the start of the window time and end of the window time, and \"explode\" between these times to create the base 48 hour blocked dataset for the cohort that subsequent data will be left joined to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Get first time on life support\n",
    "\n",
    "df = pressors.join(resp_support, on='C19_HAR_ID', how='full')\n",
    "df = df.withColumn(\"life_support_start\", f.least(f.col('pressor_life_support_start'),\n",
    "                                                 f.col('resp_life_support_start')))\n",
    "df = df.filter(f.col('life_support_start').isNotNull())\n",
    "\n",
    "df = df.withColumn('window_start', (f.col('life_support_start')-f.expr(\"INTERVAL 41 HOURS\")))\n",
    "df = df.withColumn('window_end', (f.col('life_support_start')+f.expr(\"INTERVAL 6 HOURS\")))\n",
    "df = df.select(\"C19_HAR_ID\", \"life_support_start\", \"window_start\", \"window_end\")\n",
    "\n",
    "## Re-join age at admission and disposition, admission & discharge dttm, and zip\n",
    "df = df.join(limited_ids, on='C19_HAR_ID', how='left')\n",
    "df = df.join(demo_disp, on='C19_HAR_ID', how='left')\n",
    "df = df.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \"age_at_admission\",\n",
    "               \"disposition\")\n",
    "\n",
    "df.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\", mode=\"overwrite\")\n",
    "\n",
    "## Explode between window to get all hourly timestamps\n",
    "cohort_hours = df.withColumn('txnDt', \n",
    "                                   f.explode(f.expr('sequence(window_start, window_end, interval 1 hour)')))\n",
    "cohort_hours = cohort_hours.withColumn('meas_hour', f.hour(f.col('txnDt')))\n",
    "cohort_hours = cohort_hours.withColumn('meas_date', f.to_date(f.col('txnDt')))\n",
    "cohort_hours = cohort_hours.select(\"C19_HAR_ID\", \"C19_PATIENT_ID\", \"zip_code\", \"admission_datetime\", \"discharge_datetime\", \"life_support_start\", \"window_start\", \"window_end\", \n",
    "                                   \"age_at_admission\", \"disposition\", \"meas_date\", \"meas_hour\").distinct()\n",
    "\n",
    "cohort_hours.write.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\", mode=\"overwrite\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constructing a 48 Hour Analytic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 48 Hour Labs\n",
    "\n",
    "To construct the 48 hour blocked labs dataset, we:\n",
    "1. Read in RCLIF_labs and cohort identified in previous section\n",
    "2. Left join RCLIF_labs to cohort\n",
    "3. Extract the date and hour of each lab, find minimum and maximum per hour per person per encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Read in Labs\n",
    "labs = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_labs.parquet\")\n",
    "labs = labs.withColumn(\"lab_value\",labs.lab_value.cast('double'))\n",
    "labs = labs.withColumn('lab_result_time',f.to_timestamp('lab_result_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "labs = labs.withColumn('meas_hour', f.hour(f.col('lab_result_time')))\n",
    "labs = labs.withColumn('meas_date', f.to_date(f.col('lab_result_time')))\n",
    "labs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'lab_name', 'lab_value')\n",
    "\n",
    "## Left join to cohort\n",
    "cohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\n",
    "cohort = cohort.withColumn('life_support_start_time',f.to_timestamp('life_support_start','yyyy-MM-dd HH:mm:ss'))\n",
    "cohort = cohort.select('C19_HAR_ID', 'life_support_start_time')\n",
    "\n",
    "cohort_labs = cohort.join(labs, on=\"C19_HAR_ID\", how=\"left\")\n",
    "\n",
    "cohort_labs = cohort_labs.select('C19_HAR_ID', 'life_support_start_time','meas_date', 'meas_hour', \n",
    "                                       'lab_name', 'lab_value')\n",
    "cohort_labs = cohort_labs.filter(f.col('lab_name').isNotNull())\n",
    "cohort_labs = cohort_labs.filter(f.col('lab_value').isNotNull())\n",
    "\n",
    "## Get min and max for each time lab\n",
    "group_cols = [\"C19_HAR_ID\", \"life_support_start_time\",'meas_date', 'meas_hour']\n",
    "cohort_labs_wide = cohort_labs.groupBy(group_cols) \\\n",
    "                                     .pivot(\"lab_name\") \\\n",
    "                                     .agg(f.min('lab_value').alias(\"min\"),\n",
    "                                         f.max('lab_value').alias(\"max\")).orderBy(group_cols)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we:\n",
    "\n",
    "4. Left join the hourly min and max of all the labs from the previous step to the hourly blocked cohort dataset\n",
    "5. Forward fill lab values. For this example, I have only forward filled the three lab values we wanted to use in a 48 hour analytic dataset for SIPA (creatinine, bilirubin, and platelet count). Future work will include making this a function, and filling all min and max lab values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Join to cohort hourly blocked dataset\n",
    "cohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\")\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\", 'life_support_start', 'meas_date', 'meas_hour']\n",
    "cohort_labs_48 = cohort_hours.join(cohort_labs_wide, on=group_cols, how='left')\n",
    "\n",
    "## Carry forward labs we need for SIPA\n",
    "\n",
    "cohort_labs_48 = cohort_labs_48.withColumn('billirubin_max_filled', \n",
    "                                       f.coalesce(f.col('bilirubin_total_max'), \n",
    "                                                  f.last('bilirubin_total_max', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy(group_cols)), f.lit('NULL')))\n",
    "\n",
    "\n",
    "cohort_labs_48 = cohort_labs_48.withColumn('platelet_count_min_filled', \n",
    "                                       f.coalesce(f.col('platelet_count_min'), \n",
    "                                                  f.last('platelet_count_min', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy(group_cols)), f.lit('NULL')))\n",
    "\n",
    "\n",
    "cohort_labs_48 = cohort_labs_48.withColumn('creatinine_max_filled', \n",
    "                                       f.coalesce(f.col('creatinine_max'), \n",
    "                                                  f.last('creatinine_max', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy(group_cols)), f.lit('NULL')))\n",
    "\n",
    "\n",
    "\n",
    "cohort_labs_48.write.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48.parquet\", mode=\"overwrite\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we summarize the values of the additional labs we want for SIPA (BUN, pH, potassium) and save a dataset of one observation per person/encounter with the worst values of all selected labs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Summarize to one row per person\n",
    "group_cols = ['C19_HAR_ID', 'life_support_start', 'window_start', 'window_end']\n",
    "cohort_labs_48_summary = cohort_labs_48.groupBy(group_cols) \\\n",
    "                                     .agg(f.min('creatinine_max_filled').alias(\"creatinine_max_filled\"),\n",
    "                                         f.max('platelet_count_min_filled').alias(\"platelet_count_min_filled\"),\n",
    "                                         f.max('billirubin_max_filled').alias(\"billirubin_max_filled\"),\n",
    "                                         f.max('potassium_max').alias(\"potassium_max\"),\n",
    "                                         f.max('bun_max').alias(\"bun_max\"),\n",
    "                                         f.min('ph_venous_min').alias(\"ph_venous_min\"))\\\n",
    "                                    .distinct()\\\n",
    "                                    .orderBy(\"life_support_start\")\n",
    "\n",
    "cohort_labs_48_summary.write.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48_summary.parquet\", \n",
    "                                       mode=\"overwrite\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 48 Hour Vitals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the process to create the 48 hour dataset for vitals. For vitals, we only carry forward height and weight. We also calculate MAP using the hourly SBP and DBP when MAP is not available before summarizing to one obervation per person/encounter to ensure the SBP and DBP are measured within the same hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## Read in Vitals\n",
    "vitals = spark.read.parquet(\"/project2/wparker/SIPA_data/RCLIF_vitals.parquet\")\n",
    "vitals = vitals.withColumn('measured_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "vitals = vitals.withColumn('meas_hour', f.hour(f.col('measured_time')))\n",
    "vitals = vitals.withColumn('meas_date', f.to_date(f.col('measured_time')))\n",
    "vitals = vitals.withColumn(\"vital_value\",vitals.vital_value.cast('double'))\n",
    "\n",
    "## Left join to cohort\n",
    "cohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\n",
    "cohort = cohort.withColumn('life_support_start',f.to_timestamp('life_support_start','yyyy-MM-dd HH:mm:ss'))\n",
    "cohort = cohort.withColumn('window_start',f.to_timestamp('window_start','yyyy-MM-dd HH:mm:ss'))\n",
    "cohort = cohort.withColumn('window_end',f.to_timestamp('window_end','yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "cohort_vitals = cohort.join(vitals, on=\"C19_HAR_ID\", how=\"left\")\n",
    "cohort_vitals = cohort_vitals.filter(f.col('vital_name').isNotNull())\n",
    "cohort_vitals = cohort_vitals.filter(f.col('vital_value').isNotNull())\n",
    "cohort_vitals = cohort_vitals.filter(f.col('vital_value')>=0)\n",
    "\n",
    "\n",
    "## Filter to only vitals in window\n",
    "cohort_vitals_window = cohort_vitals.filter((f.col('measured_time') >= f.col('window_start')) &\n",
    "                              (f.col('measured_time') <= f.col('window_end')))\n",
    "\n",
    "cohort_vitals_wide = cohort_vitals_window.select('C19_HAR_ID', 'life_support_start','meas_date', 'meas_hour', \n",
    "                                       'vital_name', 'vital_value')\n",
    "\n",
    "## Get min and max for each time vital\n",
    "group_cols = [\"C19_HAR_ID\", \"life_support_start\",'meas_date', 'meas_hour']\n",
    "cohort_vitals_wide = cohort_vitals_wide.groupBy(group_cols) \\\n",
    "                                     .pivot(\"vital_name\") \\\n",
    "                                     .agg(f.min('vital_value').alias(\"min\"),\n",
    "                                         f.max('vital_value').alias(\"max\")).orderBy(group_cols)\n",
    "\n",
    "\n",
    "## Join to cohort hourly blocked dataset\n",
    "cohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\")\n",
    "\n",
    "group_cols = [\"C19_HAR_ID\", 'life_support_start', 'meas_date', 'meas_hour']\n",
    "cohort_vitals_48 = cohort_hours.join(cohort_vitals_wide, on=group_cols, how='left')\n",
    "\n",
    "\n",
    "## Carry forward height and weight only for SIPA\n",
    "cohort_vitals_48 = cohort_vitals_48.withColumn('weight_filled', \n",
    "                                       f.coalesce(f.col('weight_max'), \n",
    "                                                  f.last('weight_max', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "cohort_vitals_48 = cohort_vitals_48.withColumn('height_filled', \n",
    "                                       f.coalesce(f.col('height_max'), \n",
    "                                                  f.last('height_max', True) \\\n",
    "                                                  .over(Window.partitionBy('C19_HAR_ID') \\\n",
    "                                                        .orderBy('txnDt')), f.lit('NULL')))\n",
    "\n",
    "\n",
    "## Fill in MAP using SBP and DBP--these need to be measured in same hour, so need to calculate before summarizing \n",
    "cohort_vitals_48 = cohort_vitals_48.withColumn(\"MAP_for_sofa\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN MAP_min IS NOT NULL THEN MAP_min\n",
    "        WHEN MAP_min IS NULL AND sbp_min IS NOT NULL AND dbp_min IS NOT NULL THEN ( sbp_min + 2.0 * dbp_min ) / 3.0\n",
    "        ELSE NULL\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "cohort_vitals_48.write.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48.parquet\", mode=\"overwrite\")\n",
    "\n",
    "\n",
    "## Summarize to one observation per person/encounter\n",
    "group_cols = ['C19_HAR_ID', 'life_support_start', 'window_start', 'window_end']\n",
    "\n",
    "cohort_vitals_48_summary = cohort_vitals_48.groupBy(group_cols) \\\n",
    "                                     .agg(f.max('weight_filled').alias(\"weight_filled\"),\n",
    "                                         f.max('height_filled').alias(\"height_filled\"),\n",
    "                                         f.min('MAP_for_sofa').alias(\"MAP_for_sofa\"))\\\n",
    "                                    .distinct()\\\n",
    "                                    .orderBy(\"life_support_start\")\n",
    "\n",
    "cohort_vitals_48_summary.write.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48_summary.parquet\", \n",
    "                                         mode=\"overwrite\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Combining respiratory support, vitals, and medication to determine hours of life support received"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with our base hourly blocked dataset for our cohort. We then left join the hourly blocked respiratory support and hourly blocked vitals created previously. Next, we bring in vasopressor medications and sum the number of vasopressors received per hour and whether it was dobutamine alone.\n",
    "\n",
    "We capped the number of pressors received at 4, likely because they were being transitioned off some drugs and onto others. Clinically more than 4 would not be used at one time, so we set that as the maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## Read in cohort hourly blocked base dataset\n",
    "group_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\n",
    "cohort_hours = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort_48.parquet\").orderBy(group_cols)\n",
    "\n",
    "## Bring in respiratory support hourly blocked dataset\n",
    "resp_support = spark.read.parquet(\"/project2/wparker/SIPA_data/p_f_combined_filled.parquet\").orderBy(group_cols)\n",
    "resp_support = resp_support.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'device_filled', 'fio2_filled', 'pao2_filled', 'spO2_filled', 'p_f', 's_f')\n",
    "\n",
    "## Left join respiratory support to cohort hourly blocked\n",
    "cohort_resp_support = cohort_hours.join(resp_support, on=group_cols, how=\"left\").orderBy(group_cols)\n",
    "\n",
    "# Bring in worst vitals. Only need MAP for SIPA\n",
    "vitals = spark.read.parquet(\"/project2/wparker/SIPA_data/cohort_vitals_48.parquet\")\n",
    "vitals = vitals.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'MAP_for_SOFA')\n",
    "\n",
    "cohort_resp_vitals = cohort_resp_support.join(vitals, on=group_cols, how=\"left\").orderBy(group_cols)\n",
    "\n",
    "## bring in meds to get flag for life support yes/no\n",
    "# Now pressors\n",
    "df_meds = spark.read.parquet('/project2/wparker/SIPA_data/RCLIF_medication_admin_continuous.parquet')\n",
    "df_meds = df_meds.withColumn('admin_time',f.to_timestamp('admin_dttm','yyyy-MM-dd HH:mm:ss'))\n",
    "df_meds = df_meds.withColumnRenamed(\"encounter_id\", \"C19_HAR_ID\")\n",
    "\n",
    "# Filter to pressor medications\n",
    "\n",
    "pressors = df_meds.filter(f.col('med_category')=='vasoactives')\n",
    "pressors = pressors.select(\"C19_HAR_ID\", \"admin_time\", \"med_name\")\n",
    "\n",
    "## Get cohort index times\n",
    "cohort = spark.read.parquet(\"/project2/wparker/SIPA_data/life_support_cohort.parquet\")\n",
    "cohort = cohort.select('C19_HAR_ID', 'window_start', 'window_end').distinct()\n",
    "\n",
    "## Get all pressors in 48 hour window\n",
    "cohort_pressors = cohort.join(pressors,'C19_HAR_ID','left')\n",
    "\n",
    "cohort_pressors = cohort_pressors.filter((f.col('admin_time') >= f.col('window_start')) &\n",
    "                              (f.col('admin_time') <= f.col('window_end')))\n",
    "\n",
    "cohort_pressors_48 = cohort_pressors.withColumn('meas_hour', f.hour(f.col('admin_time')))\n",
    "cohort_pressors_48 = cohort_pressors_48.withColumn('meas_date', f.to_date(f.col('admin_time')))\n",
    "\n",
    "cohort_pressors_48 = cohort_pressors_48.select('C19_HAR_ID','meas_hour','meas_date', 'med_name').distinct()\n",
    "\n",
    "## Get number of pressors per hour\n",
    "w2 = Window.partitionBy(group_cols).orderBy(\"med_name\")\n",
    "\n",
    "group_cols = ['C19_HAR_ID','meas_hour','meas_date', 'med_name']\n",
    "\n",
    "cohort_pressors_grouped = cohort_pressors_48.withColumn(\"row\",f.row_number().over(w2))\n",
    "cohort_pressors_grouped = cohort_pressors_48.withColumn(\"count\",f.lit(1))\n",
    "\n",
    "## Get count of pressors per hour\n",
    "group_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.groupBy(group_cols) \\\n",
    "                                     .pivot(\"med_name\") \\\n",
    "                                     .agg(f.count('count').alias(\"count\")).orderBy(group_cols)\n",
    "\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.withColumn(\"dobutamine_alone\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN dobutamine IS NOT NULL AND dopamine IS NULL AND epinephrine IS NULL AND isoproterenol IS NULL AND milrinone IS NULL AND norepinephrine IS NULL AND phenylephrine IS NULL AND vasopressin IS NULL THEN 1\n",
    "        ELSE 0\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "col_list = ['dobutamine', 'dopamine', 'epinephrine', 'isoproterenol', 'milrinone', 'norepinephrine', 'phenylephrine', 'vasopressin']\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.na.fill(0, subset=col_list)\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.withColumn('num_pressors', sum([f.col(c) for c in col_list]))\n",
    "\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.withColumn(\"num_pressors\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN num_pressors > 4 THEN 4\n",
    "        ELSE num_pressors\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "cohort_pressors_grouped = cohort_pressors_grouped.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'num_pressors', 'dobutamine_alone')\n",
    "\n",
    "group_cols = ['C19_HAR_ID','meas_date','meas_hour']\n",
    "cohort_resp_vitals_pressors = cohort_resp_vitals.join(cohort_pressors_grouped, on=group_cols, how=\"left\")\n",
    "\n",
    "col_list = ['num_pressors', 'dobutamine_alone']\n",
    "cohort_resp_vitals_pressors = cohort_resp_vitals_pressors.na.fill(0, subset=col_list)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to exclude people who received fewer than 6 hours of life support. So we create a flag for if someone received life support in that hour, defined as receiving vasopressor medications, non-invasive or invasive mechanical ventilation, a PaO2/FiO2 ratio less than 200, or an SpO2/FiO2 ratio less than 179. We sum and filter out those receiveing less than 6 hours of life support."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "##Flag if on life support at each hour\n",
    "\n",
    "cohort_resp_vitals_pressors = cohort_resp_vitals_pressors.withColumn(\"on_life_support\", f.expr(\n",
    "        \"\"\"\n",
    "        CASE\n",
    "        WHEN num_pressors >=1 THEN 1\n",
    "        WHEN device_filled == 'NIPPV' THEN 1\n",
    "        WHEN device_filled == 'Vent' THEN 1\n",
    "        WHEN  p_f < 200 THEN 1\n",
    "        WHEN s_f < 179 THEN 1\n",
    "        ELSE 0\n",
    "        END\n",
    "        \"\"\"\n",
    "    ))\n",
    "\n",
    "cohort_life_support_count = cohort_resp_vitals_pressors.groupBy('C19_HAR_ID') \\\n",
    "                                            .agg(f.sum('on_life_support').alias('life_support_count')) \\\n",
    "                                            .filter(f.col('life_support_count')>=6)\n",
    "\n",
    "df = cohort_life_support_count.join(cohort_resp_vitals_pressors, on='C19_HAR_ID', how=\"left\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merging scores, labs, and dialysis flag\n",
    "\n",
    "Lastly, we repeat a similar procedure to left join hourly blocked minimum scores, labs, and a flag for whether a person was receiving dialysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "## add glasgow score\n",
    "scores = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_scores_10192023.csv')\n",
    "scores = scores.withColumn('score_time',f.to_timestamp('score_time','yyyy-MM-dd HH:mm:ss'))\n",
    "scores = scores.withColumn('meas_hour', f.hour(f.col('score_time')))\n",
    "scores = scores.withColumn('meas_date', f.to_date(f.col('score_time')))\n",
    "scores = scores.withColumn(\"score_value\",scores.score_value.cast('double'))\n",
    "\n",
    "## Get min GCS\n",
    "group_cols = [\"C19_HAR_ID\",'meas_date', 'meas_hour']\n",
    "scores_wide = scores.groupBy(group_cols) \\\n",
    "                                     .pivot(\"score_name\") \\\n",
    "                                     .agg(f.min('score_value').alias(\"min\")).orderBy(group_cols)\n",
    "scores_wide = scores_wide.withColumnRenamed('NUR RA GLASGOW ADULT SCORING', 'min_gcs_score')\n",
    "\n",
    "cohort_scores_48 = cohort_hours.join(scores_wide, on=group_cols, how=\"left\")\n",
    "cohort_scores_48 = cohort_scores_48.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'min_gcs_score')\n",
    "\n",
    "df = df.join(cohort_scores_48, on=group_cols, how=\"left\")\n",
    "\n",
    "## Add labs\n",
    "labs = spark.read.parquet(\"/project2/wparker/SIPA_data/cohort_labs_48.parquet\")\n",
    "labs = labs.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'billirubin_max_filled', 'potassium_max', 'bun_max', 'ph_venous_min',\n",
    "                         'platelet_count_min_filled', 'creatinine_max_filled')\n",
    "df = df.join(labs, on=group_cols, how=\"left\")\n",
    "\n",
    "## add dialysis flag\n",
    "dialysis = spark.read.option(\"header\",True).csv('/project2/wparker/SIPA_data/RCLIF_dialysis_flag_only.csv')\n",
    "dialysis = dialysis.withColumn('recorded_time',f.to_timestamp('recorded_time','yyyy-MM-dd HH:mm:ss'))\n",
    "dialysis = dialysis.withColumn('meas_hour', f.hour(f.col('recorded_time')))\n",
    "dialysis = dialysis.withColumn('meas_date', f.to_date(f.col('recorded_time')))\n",
    "\n",
    "dialysis = dialysis.select('C19_HAR_ID', 'meas_date', 'meas_hour', 'on_dialysis') \\\n",
    "                .distinct() \\\n",
    "                .groupBy(group_cols) \\\n",
    "                .agg(f.max('on_dialysis').alias('on_dialysis'))\n",
    "df = df.join(dialysis, on=group_cols, how=\"left\")\n",
    "\n",
    "df.repartition(1).write.csv(\"/project2/wparker/SIPA_data/sipa_df_48.csv\", mode='overwrite', header=\"true\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
